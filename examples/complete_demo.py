#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®°å¿†ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹ - ä».envè¯»å–é…ç½®å¹¶è¿è¡Œ
æ‰€æœ‰é…ç½®é€šè¿‡.envæ–‡ä»¶ç®¡ç†ï¼Œæ— éœ€å‘½ä»¤è¡Œå‚æ•°
"""
import sys
import os
import json
from pathlib import Path
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'src'))
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()
from tinymem0 import MemorySystem


def str_to_bool(value: str) -> bool:
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸ƒå°”å€¼"""
    if not value:
        return False
    return value.lower() in ('true', '1', 'yes', 'on')


def check_model_in_registry(shortcut, format_type, quantization):
    """æ£€æŸ¥æ¨¡å‹æ³¨å†Œè¡¨ï¼Œè¿”å›æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    registry_file = Path(__file__).parent.parent / 'model_downloaded.json'
    
    if not registry_file.exists():
        return None
    
    try:
        with open(registry_file, 'r', encoding='utf-8') as f:
            registry = json.load(f)
        
        for model in registry.get('models', []):
            if (model['shortcut'] == shortcut and 
                model['format'] == format_type and 
                model['quantization'] == quantization):
                # æ‰¾åˆ°åŒ¹é…çš„é…ç½®ï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                local_path = model['local_path']
                if Path(local_path).exists():
                    return local_path
                else:
                    # é…ç½®å­˜åœ¨ä½†æ–‡ä»¶ä¸¢å¤±
                    return None
        return None
    except Exception as e:
        print(f"   âš ï¸  è¯»å–æ¨¡å‹æ³¨å†Œè¡¨å¤±è´¥: {e}")
        return None


def check_embedding_in_registry(model_id, embedding_dim):
    """æ£€æŸ¥åµŒå…¥æ¨¡å‹æ³¨å†Œè¡¨ï¼Œè¿”å›æœ¬åœ°è·¯å¾„ï¼ˆå¦‚æœå­˜åœ¨ï¼‰"""
    registry_file = Path(__file__).parent.parent / 'model_downloaded.json'
    
    if not registry_file.exists():
        return None
    
    try:
        with open(registry_file, 'r', encoding='utf-8') as f:
            registry = json.load(f)
        
        for model in registry.get('embedding_models', []):
            if (model['model_id'] == model_id and 
                model['embedding_dim'] == embedding_dim):
                # æ‰¾åˆ°åŒ¹é…çš„é…ç½®ï¼Œæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                local_path = model['local_path']
                if Path(local_path).exists():
                    return local_path
                else:
                    return None
        return None
    except Exception as e:
        print(f"   âš ï¸  è¯»å–åµŒå…¥æ¨¡å‹æ³¨å†Œè¡¨å¤±è´¥: {e}")
        return None


def add_embedding_to_registry(model_id, embedding_dim, local_path):
    """å°†åµŒå…¥æ¨¡å‹æ·»åŠ åˆ°æ³¨å†Œè¡¨"""
    registry_file = Path(__file__).parent.parent / 'model_downloaded.json'
    
    # è¯»å–ç°æœ‰æ³¨å†Œè¡¨
    if registry_file.exists():
        try:
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry = json.load(f)
        except:
            registry = {"_description": "æœ¬åœ°æ¨¡å‹æ³¨å†Œè¡¨", "models": [], "embedding_models": []}
    else:
        registry = {"_description": "æœ¬åœ°æ¨¡å‹æ³¨å†Œè¡¨", "models": [], "embedding_models": []}
    
    # ç¡®ä¿æœ‰embedding_modelså­—æ®µ
    if 'embedding_models' not in registry:
        registry['embedding_models'] = []
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    for model in registry['embedding_models']:
        if (model['model_id'] == model_id and 
            model['embedding_dim'] == embedding_dim):
            # æ›´æ–°ç°æœ‰è®°å½•
            model['local_path'] = local_path
            break
    else:
        # æ·»åŠ æ–°è®°å½•
        registry['embedding_models'].append({
            "model_id": model_id,
            "embedding_dim": embedding_dim,
            "local_path": local_path
        })
    
    # ä¿å­˜æ³¨å†Œè¡¨
    try:
        with open(registry_file, 'w', encoding='utf-8') as f:
            json.dump(registry, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"   âš ï¸  ä¿å­˜åµŒå…¥æ¨¡å‹æ³¨å†Œè¡¨å¤±è´¥: {e}")


def add_model_to_registry(shortcut, format_type, quantization, local_path, model_id):
    """å°†æ¨¡å‹æ·»åŠ åˆ°æ³¨å†Œè¡¨"""
    registry_file = Path(__file__).parent.parent / 'model_downloaded.json'
    
    # è¯»å–ç°æœ‰æ³¨å†Œè¡¨
    if registry_file.exists():
        try:
            with open(registry_file, 'r', encoding='utf-8') as f:
                registry = json.load(f)
        except:
            registry = {"_description": "æœ¬åœ°æ¨¡å‹æ³¨å†Œè¡¨", "models": []}
    else:
        registry = {"_description": "æœ¬åœ°æ¨¡å‹æ³¨å†Œè¡¨", "models": []}
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    for model in registry['models']:
        if (model['shortcut'] == shortcut and 
            model['format'] == format_type and 
            model['quantization'] == quantization):
            # æ›´æ–°ç°æœ‰è®°å½•
            model['local_path'] = local_path
            model['model_id'] = model_id
            break
    else:
        # æ·»åŠ æ–°è®°å½•
        registry['models'].append({
            "shortcut": shortcut,
            "format": format_type,
            "quantization": quantization,
            "local_path": local_path,
            "model_id": model_id
        })
    
    # ä¿å­˜æ³¨å†Œè¡¨
    try:
        with open(registry_file, 'w', encoding='utf-8') as f:
            json.dump(registry, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"   âš ï¸  ä¿å­˜æ¨¡å‹æ³¨å†Œè¡¨å¤±è´¥: {e}")


def download_models():
    """ä».envè¯»å–é…ç½®å¹¶ä¸‹è½½æ¨¡å‹"""
    # è¯»å–é…ç½®
    use_local_llm = str_to_bool(os.getenv('USE_LOCAL_LLM', 'false'))
    skip_download = str_to_bool(os.getenv('SKIP_DOWNLOAD', 'false'))
    model_shortcut = os.getenv('MODEL_SHORTCUT', 'mistral-7b')
    model_format = os.getenv('MODEL_FORMAT', 'gguf')
    quantization = os.getenv('MODEL_QUANTIZATION', 'Q4_K_M')
    embedding_model = os.getenv('LOCAL_EMBEDDING_MODEL', 'BAAI/bge-small-zh-v1.5')
    hf_token = os.getenv('HF_TOKEN')  # ä».envè¯»å–HuggingFaceä»¤ç‰Œ
    
    print("=" * 70)
    print("ğŸ“¦ æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹")
    print("=" * 70)
    print(f"\né…ç½®ä¿¡æ¯ï¼ˆæ¥è‡ª .envï¼‰:")
    print(f"  USE_LOCAL_LLM: {use_local_llm}")
    print(f"  MODEL_SHORTCUT: {model_shortcut}")
    print(f"  MODEL_FORMAT: {model_format}")
    print(f"  MODEL_QUANTIZATION: {quantization}")
    print(f"  SKIP_DOWNLOAD: {skip_download}")
    
    # 1. ä¸‹è½½åµŒå…¥æ¨¡å‹ï¼ˆè°ƒç”¨åº•å±‚å·¥å…·ï¼‰
    print("\n1ï¸âƒ£ åµŒå…¥æ¨¡å‹...")
    print(f"   æ¨¡å‹: {embedding_model}")
    
    # å…ˆæ£€æŸ¥åµŒå…¥æ¨¡å‹æ³¨å†Œè¡¨
    embedding_dim_val = int(os.getenv('EMBEDDING_DIM', '512'))
    print(f"   ğŸ” æ£€æŸ¥åµŒå…¥æ¨¡å‹æ³¨å†Œè¡¨...")
    embedding_path = check_embedding_in_registry(embedding_model, embedding_dim_val)
    
    if embedding_path:
        print(f"   âœ… åœ¨æ³¨å†Œè¡¨ä¸­æ‰¾åˆ°åµŒå…¥æ¨¡å‹")
        print(f"   ğŸ“‚ ä½ç½®: {embedding_path}")
        print(f"   â­ï¸  è·³è¿‡ä¸‹è½½")
    else:
        print(f"   â„¹ï¸  æ³¨å†Œè¡¨ä¸­æ— æ­¤é…ç½®ï¼Œéœ€è¦ä¸‹è½½")
        sys.path.insert(0, str(Path(__file__).parent.parent / 'utils'))
        from model_manager.downloader import download_embedding_model
        
        try:
            downloaded_path = download_embedding_model(model_id=embedding_model)
            print(f"   âœ… åµŒå…¥æ¨¡å‹å°±ç»ª")
            print(f"   ğŸ“‚ ä½ç½®: {downloaded_path}")
            
            # æ·»åŠ åˆ°æ³¨å†Œè¡¨
            print(f"   ğŸ’¾ æ›´æ–°åµŒå…¥æ¨¡å‹æ³¨å†Œè¡¨...")
            add_embedding_to_registry(embedding_model, embedding_dim_val, downloaded_path)
        except Exception as e:
            print(f"   âš ï¸  åµŒå…¥æ¨¡å‹é¢„ä¸‹è½½å¤±è´¥ï¼ˆé¦–æ¬¡ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰: {e}")
    
    # 2. æ£€æŸ¥LLMæ¨¡å‹
    print("\n2ï¸âƒ£ LLMæ¨¡å‹...")
    
    if not use_local_llm:
        print("   â­ï¸  äº‘ç«¯APIæ¨¡å¼ï¼Œæ— éœ€ä¸‹è½½")
        print("\n" + "=" * 70)
        return None
    
    if skip_download:
        print("   â­ï¸  å·²è®¾ç½® SKIP_DOWNLOAD=trueï¼Œè·³è¿‡ä¸‹è½½")
        print("\n" + "=" * 70)
        return None
    
    # å…ˆæ£€æŸ¥æ¨¡å‹æ³¨å†Œè¡¨
    print(f"   ğŸ” æ£€æŸ¥æ¨¡å‹æ³¨å†Œè¡¨...")
    registry_path = check_model_in_registry(model_shortcut, model_format, quantization)
    
    if registry_path:
        print(f"   âœ… åœ¨æ³¨å†Œè¡¨ä¸­æ‰¾åˆ°æ¨¡å‹")
        print(f"   ğŸ“‚ ä½ç½®: {registry_path}")
        print(f"   â­ï¸  è·³è¿‡ä¸‹è½½")
        print("\n" + "=" * 70)
        return registry_path
    else:
        print(f"   â„¹ï¸  æ³¨å†Œè¡¨ä¸­æ— æ­¤é…ç½®ï¼Œéœ€è¦ä¸‹è½½")
    
    # è°ƒç”¨åº•å±‚ä¸‹è½½å·¥å…·
    sys.path.insert(0, str(Path(__file__).parent.parent / 'utils'))
    from model_manager import download_llm_model_with_shortcut, _load_model_shortcuts
    
    try:
        # è·å–æ¨¡å‹IDï¼ˆç”¨äºæ³¨å†Œè¡¨ï¼‰
        shortcuts = _load_model_shortcuts()
        if model_shortcut in shortcuts:
            model_info = shortcuts[model_shortcut]
            model_id = model_info.get(model_format, model_shortcut)
        else:
            model_id = model_shortcut
        
        downloaded_path = download_llm_model_with_shortcut(
            model_shortcut=model_shortcut,
            model_format=model_format,
            quantization=quantization,
            verbose=True,
            hf_token=hf_token  # ä¼ é€’HFä»¤ç‰Œåˆ°ä¸‹å±‚
        )
        
        print(f"\n   âœ… æ¨¡å‹å°±ç»ª")
        print(f"   ğŸ“‚ ä½ç½®: {downloaded_path}")
        
        # æ·»åŠ åˆ°æ³¨å†Œè¡¨
        print(f"   ğŸ’¾ æ›´æ–°æ¨¡å‹æ³¨å†Œè¡¨...")
        add_model_to_registry(model_shortcut, model_format, quantization, downloaded_path, model_id)
        
        print("\n" + "=" * 70)
        return downloaded_path
        
    except Exception as e:
        print(f"\n   âŒ ä¸‹è½½å¤±è´¥: {e}")
        print("   ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹")
        print("\n" + "=" * 70)
        return None


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºè®°å¿†ç³»ç»Ÿçš„ä½¿ç”¨ï¼ˆä».envè¯»å–æ‰€æœ‰é…ç½®ï¼‰"""
    # ä».envè¯»å–é…ç½®
    use_local_llm = str_to_bool(os.getenv('USE_LOCAL_LLM', 'false'))
    model_shortcut = os.getenv('MODEL_SHORTCUT', 'mistral-7b')
    model_format = os.getenv('MODEL_FORMAT', 'gguf')
    quantization = os.getenv('MODEL_QUANTIZATION', 'Q4_K_M')
    local_embedding_model = os.getenv('LOCAL_EMBEDDING_MODEL', 'BAAI/bge-small-zh-v1.5')
    embedding_dim_str = os.getenv('EMBEDDING_DIM', '')
    
    # è‡ªåŠ¨æ¨å¯¼æœ¬åœ°æ¨¡å‹è·¯å¾„ï¼ˆä¸é…ç½®ä¿æŒä¸€è‡´ï¼‰
    if use_local_llm:
        # æ ¹æ®é…ç½®è‡ªåŠ¨ç”Ÿæˆæ¨¡å‹è·¯å¾„
        sys.path.insert(0, str(Path(__file__).parent.parent / 'utils'))
        from model_manager import _load_model_shortcuts
        
        shortcuts = _load_model_shortcuts()
        if model_shortcut in shortcuts:
            model_info = shortcuts[model_shortcut]
            model_id = model_info.get(model_format, model_shortcut)
            
            if model_format == 'gguf':
                # GGUFæ ¼å¼ï¼šmodels/gguf/æ–‡ä»¶å.gguf
                model_name = model_id.split('/')[-1]  # æå–ä»“åº“å
                # ä»ä»“åº“åæå–åŸºç¡€æ¨¡å‹åï¼ˆç§»é™¤-GGUFåç¼€ï¼‰
                base_name = model_name.replace('-GGUF', '').lower()
                filename = f"{base_name}.{quantization}.gguf"
                local_model_path = f"models/gguf/{filename}"
            else:
                # SafeTensorsæ ¼å¼ï¼šmodels/safetensors/model_id/
                local_model_path = f"models/safetensors/{model_id}"
        else:
            local_model_path = ""
    else:
        local_model_path = ""
    
    # è‡ªåŠ¨è®¾ç½®embedding_dim
    if embedding_dim_str:
        embedding_dim = int(embedding_dim_str)
    else:
        embedding_dim = 512 if use_local_llm else 1536
    
    # éªŒè¯é…ç½®
    if not use_local_llm and not os.getenv("DASHSCOPE_API_KEY"):
        raise RuntimeError(
            "ä½¿ç”¨äº‘ç«¯APIéœ€è¦é…ç½® DASHSCOPE_API_KEY\n"
            "è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: DASHSCOPE_API_KEY=your_api_key_here"
        )
    
    mode = "æœ¬åœ°æ¨¡å‹" if use_local_llm else "äº‘ç«¯API"
    if use_local_llm and local_model_path:
        print(f"åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ ({mode}: {local_model_path})...")
    else:
        print(f"åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ ({mode})...")
    
    memory_system = MemorySystem(
        use_local_llm=use_local_llm,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # ç¤ºä¾‹1: å†™å…¥è®°å¿†
    print("\n=== ç¤ºä¾‹1: å†™å…¥è®°å¿† ===")
    conversation1 = "Hello, my name is John. I'm a software engineer and I love watching movies, especially science fiction."
    print(f"ç”¨æˆ·å¯¹è¯: {conversation1}")
    
    memory_system.write_memory(
        conversation=conversation1,
        user_id="user_001",
        agent_id="agent_001"
    )
    
    # ç¤ºä¾‹2: æœç´¢è®°å¿†
    print("\n=== ç¤ºä¾‹2: æœç´¢è®°å¿† ===")
    query = "What is John's occupation?"
    print(f"æœç´¢æŸ¥è¯¢: {query}")
    
    results = memory_system.search_memory(
        query=query,
        user_id="user_001",
        agent_id="agent_001",
        limit=3
    )
    
    print("æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['text']} (ç›¸ä¼¼åº¦: {result['score']:.3f})")
    
    # ç¤ºä¾‹3: æ›´æ–°è®°å¿†
    print("\n=== ç¤ºä¾‹3: æ›´æ–°è®°å¿† ===")
    conversation2 = "I recently changed my career. I'm now a product manager and no longer a software engineer."
    print(f"ç”¨æˆ·å¯¹è¯: {conversation2}")
    
    memory_system.write_memory(
        conversation=conversation2,
        user_id="user_001",
        agent_id="agent_001"
    )
    
    # å†æ¬¡æœç´¢éªŒè¯æ›´æ–°
    print("\næ›´æ–°åå†æ¬¡æœç´¢:")
    results = memory_system.search_memory(
        query="John's occupation",
        user_id="user_001",
        agent_id="agent_001",
        limit=3
    )
    
    print("æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['text']} (ç›¸ä¼¼åº¦: {result['score']:.3f})")


def example_memory_update():
    """ç¤ºä¾‹2: è®°å¿†æ›´æ–°å’Œå†²çªå¤„ç†"""
    print("\n" + "=" * 70)
    print("ğŸ”„ ç¤ºä¾‹2: è®°å¿†æ›´æ–°")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_update",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # åˆå§‹è®°å¿†
    print("\n1ï¸âƒ£ å†™å…¥åˆå§‹ä¿¡æ¯...")
    memory.write_memory(
        "My name is Mike, I'm 25 years old, and I work as a product manager in Shanghai",
        user_id="user_mike",
        agent_id="assistant"
    )
    print("âœ… åˆå§‹è®°å¿†å·²ä¿å­˜")
    
    # æœç´¢å½“å‰èŒä¸š
    print("\n2ï¸âƒ£ æŸ¥è¯¢å½“å‰èŒä¸š...")
    results = memory.search_memory(
        "Mike's occupation",
        user_id="user_mike",
        limit=1
    )
    if results:
        print(f"  å½“å‰èŒä¸š: {results[0]['text']}")
    
    # æ›´æ–°ä¿¡æ¯
    print("\n3ï¸âƒ£ æ›´æ–°èŒä¸šä¿¡æ¯...")
    memory.write_memory(
        "I changed jobs and became a data scientist",
        user_id="user_mike",
        agent_id="assistant"
    )
    print("âœ… è®°å¿†å·²æ›´æ–°")
    
    # å†æ¬¡æœç´¢
    print("\n4ï¸âƒ£ éªŒè¯æ›´æ–°ç»“æœ...")
    results = memory.search_memory(
        "Mike's occupation",
        user_id="user_mike",
        limit=2
    )
    
    print("  æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"    {i}. {result['text']}")
    
    print("\nâœ… ç¤ºä¾‹2å®Œæˆ")


def example_multi_user():
    """ç¤ºä¾‹3: å¤šç”¨æˆ·è®°å¿†éš”ç¦»"""
    print("\n" + "=" * 70)
    print("ğŸ‘¥ ç¤ºä¾‹3: å¤šç”¨æˆ·åœºæ™¯")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_multiuser",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # ç”¨æˆ·Açš„è®°å¿†
    print("\n1ï¸âƒ£ ç”¨æˆ·Açš„å¯¹è¯...")
    memory.write_memory(
        "I love Sichuan cuisine, especially mapo tofu",
        user_id="user_a",
        agent_id="assistant"
    )
    
    # ç”¨æˆ·Bçš„è®°å¿†
    print("2ï¸âƒ£ ç”¨æˆ·Bçš„å¯¹è¯...")
    memory.write_memory(
        "I love Cantonese cuisine, especially white cut chicken",
        user_id="user_b",
        agent_id="assistant"
    )
    
    # åˆ†åˆ«æœç´¢
    print("\n3ï¸âƒ£ åˆ†åˆ«æœç´¢ç”¨æˆ·åå¥½...")
    
    print("  ç”¨æˆ·Açš„æœç´¢ç»“æœ:")
    results_a = memory.search_memory(
        "What cuisine do you like",
        user_id="user_a",
        limit=1
    )
    for r in results_a:
        print(f"    - {r['text']}")
    
    print("  ç”¨æˆ·Bçš„æœç´¢ç»“æœ:")
    results_b = memory.search_memory(
        "What cuisine do you like",
        user_id="user_b",
        limit=1
    )
    for r in results_b:
        print(f"    - {r['text']}")
    
    print("\nâœ… ç¤ºä¾‹3å®Œæˆ - è®°å¿†å·²æ­£ç¡®éš”ç¦»")


def example_fact_extraction():
    """ç¤ºä¾‹4: äº‹å®æå–åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ç¤ºä¾‹4: äº‹å®æå–")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_facts",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„å¯¹è¯
    test_cases = [
        ("ç®€å•é—®å€™", "Hello"),
        ("ä¸ªäººä¿¡æ¯", "My name is Alice, I'm 30 years old, and I live in Shenzhen"),
        ("å…´è¶£çˆ±å¥½", "I love traveling. I visited Japan and Korea last year"),
        ("å·¥ä½œä¿¡æ¯", "I work as a UI designer at an internet company"),
        ("æœªæ¥è®¡åˆ’", "I plan to learn photography next year")
    ]
    
    print("\næµ‹è¯•äº‹å®æå–åŠŸèƒ½:\n")
    for label, conversation in test_cases:
        print(f"  [{label}]")
        print(f"  å¯¹è¯: {conversation}")
        
        facts = memory.extract_facts(conversation)
        
        if facts:
            print(f"  æå–äº‹å®: {facts}")
        else:
            print("  æå–äº‹å®: (æ— å®è´¨æ€§ä¿¡æ¯)")
        print()
    
    print("âœ… ç¤ºä¾‹4å®Œæˆ")


def example_advanced_search():
    """ç¤ºä¾‹5: é«˜çº§æœç´¢åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("ğŸ” ç¤ºä¾‹5: é«˜çº§æœç´¢")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_search",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # å‡†å¤‡ä¸°å¯Œçš„è®°å¿†æ•°æ®
    print("\n1ï¸âƒ£ å‡†å¤‡æµ‹è¯•æ•°æ®...")
    knowledge = [
        "I graduated from Tsinghua University with a degree in Computer Science in 2020",
        "My first job was as a backend developer at ByteDance",
        "I switched to Alibaba in 2022",
        "I'm currently responsible for developing e-commerce recommendation systems",
        "My strongest tech stack is Python and Go",
        "In my spare time, I enjoy researching machine learning algorithms",
        "My long-term goal is to become a technical expert"
    ]
    
    for k in knowledge:
        memory.write_memory(k, user_id="user_tech", agent_id="assistant")
    
    print(f"âœ… å·²å†™å…¥ {len(knowledge)} æ¡è®°å¿†")
    
    # ä¸åŒç±»å‹çš„æœç´¢
    print("\n2ï¸âƒ£ æ‰§è¡Œä¸åŒç±»å‹çš„æœç´¢...\n")
    
    search_cases = [
        ("æ•™è‚²èƒŒæ™¯", "university graduation"),
        ("å·¥ä½œç»å†", "job history"),
        ("æŠ€æœ¯èƒ½åŠ›", "programming languages expertise"),
        ("å…´è¶£çˆ±å¥½", "hobbies"),
        ("èŒä¸šè§„åˆ’", "career goals")
    ]
    
    for category, query in search_cases:
        print(f"  [{category}] æŸ¥è¯¢: {query}")
        results = memory.search_memory(
            query=query,
            user_id="user_tech",
            limit=2
        )
        
        if results:
            for i, r in enumerate(results, 1):
                print(f"    {i}. {r['text']} (åˆ†æ•°: {r['score']:.3f})")
        else:
            print("    æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        print()
    
    print("âœ… ç¤ºä¾‹5å®Œæˆ")


if __name__ == "__main__":
    print("=" * 70)
    print("TinyMem0 è®°å¿†ç³»ç»Ÿå®Œæ•´ç¤ºä¾‹")
    print("é…ç½®æ¥æº: .env æ–‡ä»¶")
    print("=" * 70)
    
    # ä¸‹è½½æ¨¡å‹ï¼ˆæ ¹æ®.envé…ç½®ï¼‰
    downloaded_path = download_models()
    
    # å¦‚æœä¸‹è½½äº†æ¨¡å‹ï¼Œæ›´æ–°.envä¸­çš„LOCAL_MODEL_PATH
    if downloaded_path:
        env_file = Path(__file__).parent.parent / '.env'
        if env_file.exists():
            lines = []
            updated = False
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith('LOCAL_MODEL_PATH='):
                        lines.append(f'LOCAL_MODEL_PATH={downloaded_path}\n')
                        updated = True
                    else:
                        lines.append(line)
            
            if updated:
                with open(env_file, 'w', encoding='utf-8') as f:
                    f.writelines(lines)
                print(f"\nâœ… å·²æ›´æ–° .env: LOCAL_MODEL_PATH={downloaded_path}\n")
                # é‡æ–°åŠ è½½.env
                load_dotenv(override=True)
    
    # è¿è¡Œä¸»ç¤ºä¾‹
    main()
