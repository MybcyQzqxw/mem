#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®°å¿†ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹ - ä».envè¯»å–é…ç½®å¹¶è¿è¡Œ
æ‰€æœ‰é…ç½®é€šè¿‡.envæ–‡ä»¶ç®¡ç†ï¼Œæ— éœ€å‘½ä»¤è¡Œå‚æ•°
"""
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'src'))
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()
from tinymem0 import MemorySystem


def str_to_bool(value: str) -> bool:
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸ƒå°”å€¼"""
    if not value:
        return False
    return value.lower() in ('true', '1', 'yes', 'on')


def download_models():
    """ä».envè¯»å–é…ç½®å¹¶ä¸‹è½½æ¨¡å‹"""
    # è¯»å–é…ç½®
    use_local_llm = str_to_bool(os.getenv('USE_LOCAL_LLM', 'false'))
    skip_download = str_to_bool(os.getenv('SKIP_DOWNLOAD', 'false'))
    model_shortcut = os.getenv('MODEL_SHORTCUT', 'mistral-7b')
    model_format = os.getenv('MODEL_FORMAT', 'gguf')
    quantization = os.getenv('MODEL_QUANTIZATION', 'Q4_K_M')
    embedding_model = os.getenv('LOCAL_EMBEDDING_MODEL', 'BAAI/bge-small-zh-v1.5')
    hf_token = os.getenv('HF_TOKEN')  # ä».envè¯»å–HuggingFaceä»¤ç‰Œ
    
    print("=" * 70)
    print("ğŸ“¦ æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹")
    print("=" * 70)
    print(f"\né…ç½®ä¿¡æ¯ï¼ˆæ¥è‡ª .envï¼‰:")
    print(f"  USE_LOCAL_LLM: {use_local_llm}")
    print(f"  MODEL_SHORTCUT: {model_shortcut}")
    print(f"  MODEL_FORMAT: {model_format}")
    print(f"  MODEL_QUANTIZATION: {quantization}")
    print(f"  SKIP_DOWNLOAD: {skip_download}")
    
    # 1. ä¸‹è½½åµŒå…¥æ¨¡å‹ï¼ˆè°ƒç”¨åº•å±‚å·¥å…·ï¼‰
    print("\n1ï¸âƒ£ åµŒå…¥æ¨¡å‹...")
    print(f"   æ¨¡å‹: {embedding_model}")
    
    sys.path.insert(0, str(Path(__file__).parent.parent / 'utils'))
    from model_manager.downloader import download_embedding_model
    
    try:
        download_embedding_model(model_id=embedding_model)
    except Exception as e:
        print(f"   âš ï¸  åµŒå…¥æ¨¡å‹é¢„ä¸‹è½½å¤±è´¥ï¼ˆé¦–æ¬¡ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨ä¸‹è½½ï¼‰: {e}")
    
    # 2. æ£€æŸ¥LLMæ¨¡å‹
    print("\n2ï¸âƒ£ LLMæ¨¡å‹...")
    
    if not use_local_llm:
        print("   â­ï¸  äº‘ç«¯APIæ¨¡å¼ï¼Œæ— éœ€ä¸‹è½½")
        print("\n" + "=" * 70)
        return None
    
    if skip_download:
        print("   â­ï¸  å·²è®¾ç½® SKIP_DOWNLOAD=trueï¼Œè·³è¿‡ä¸‹è½½")
        print("\n" + "=" * 70)
        return None
    
    # è°ƒç”¨åº•å±‚ä¸‹è½½å·¥å…·
    sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))
    from download_llm import download_model_with_shortcut
    
    try:
        downloaded_path = download_model_with_shortcut(
            model_shortcut=model_shortcut,
            model_format=model_format,
            quantization=quantization,
            verbose=True,
            hf_token=hf_token  # ä¼ é€’HFä»¤ç‰Œåˆ°ä¸‹å±‚
        )
        
        print(f"\n   âœ… æ¨¡å‹å°±ç»ª")
        print(f"   ğŸ“‚ ä½ç½®: {downloaded_path}")
        print("\n" + "=" * 70)
        return downloaded_path
        
    except Exception as e:
        print(f"\n   âŒ ä¸‹è½½å¤±è´¥: {e}")
        print("   ğŸ’¡ è¯·æ£€æŸ¥ç½‘ç»œè¿æ¥æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹")
        print("\n" + "=" * 70)
        return None


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºè®°å¿†ç³»ç»Ÿçš„ä½¿ç”¨ï¼ˆä».envè¯»å–æ‰€æœ‰é…ç½®ï¼‰"""
    # ä».envè¯»å–é…ç½®
    use_local_llm = str_to_bool(os.getenv('USE_LOCAL_LLM', 'false'))
    local_model_path = os.getenv('LOCAL_MODEL_PATH', '')
    local_embedding_model = os.getenv('LOCAL_EMBEDDING_MODEL', 'BAAI/bge-small-zh-v1.5')
    embedding_dim_str = os.getenv('EMBEDDING_DIM', '')
    
    # è‡ªåŠ¨è®¾ç½®embedding_dim
    if embedding_dim_str:
        embedding_dim = int(embedding_dim_str)
    else:
        embedding_dim = 512 if use_local_llm else 1536
    
    # éªŒè¯é…ç½®
    if not use_local_llm and not os.getenv("DASHSCOPE_API_KEY"):
        raise RuntimeError(
            "ä½¿ç”¨äº‘ç«¯APIéœ€è¦é…ç½® DASHSCOPE_API_KEY\n"
            "è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: DASHSCOPE_API_KEY=your_api_key_here"
        )
    
    mode = "æœ¬åœ°æ¨¡å‹" if use_local_llm else "äº‘ç«¯API"
    if use_local_llm and local_model_path:
        print(f"åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ ({mode}: {local_model_path})...")
    else:
        print(f"åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ ({mode})...")
    
    memory_system = MemorySystem(
        use_local_llm=use_local_llm,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # ç¤ºä¾‹1: å†™å…¥è®°å¿†
    print("\n=== ç¤ºä¾‹1: å†™å…¥è®°å¿† ===")
    conversation1 = "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ï¼Œæ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæˆ‘å–œæ¬¢çœ‹ç”µå½±ï¼Œç‰¹åˆ«æ˜¯ç§‘å¹»ç‰‡ã€‚"
    print(f"ç”¨æˆ·å¯¹è¯: {conversation1}")
    
    memory_system.write_memory(
        conversation=conversation1,
        user_id="user_001",
        agent_id="agent_001"
    )
    
    # ç¤ºä¾‹2: æœç´¢è®°å¿†
    print("\n=== ç¤ºä¾‹2: æœç´¢è®°å¿† ===")
    query = "å¼ ä¸‰çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"æœç´¢æŸ¥è¯¢: {query}")
    
    results = memory_system.search_memory(
        query=query,
        user_id="user_001",
        agent_id="agent_001",
        limit=3
    )
    
    print("æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['text']} (ç›¸ä¼¼åº¦: {result['score']:.3f})")
    
    # ç¤ºä¾‹3: æ›´æ–°è®°å¿†
    print("\n=== ç¤ºä¾‹3: æ›´æ–°è®°å¿† ===")
    conversation2 = "æˆ‘æœ€è¿‘æ”¹å˜äº†èŒä¸šï¼Œç°åœ¨æ˜¯ä¸€åäº§å“ç»ç†ï¼Œä¸å†åšè½¯ä»¶å·¥ç¨‹å¸ˆäº†ã€‚"
    print(f"ç”¨æˆ·å¯¹è¯: {conversation2}")
    
    memory_system.write_memory(
        conversation=conversation2,
        user_id="user_001",
        agent_id="agent_001"
    )
    
    # å†æ¬¡æœç´¢éªŒè¯æ›´æ–°
    print("\næ›´æ–°åå†æ¬¡æœç´¢:")
    results = memory_system.search_memory(
        query="å¼ ä¸‰çš„èŒä¸š",
        user_id="user_001",
        agent_id="agent_001",
        limit=3
    )
    
    print("æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['text']} (ç›¸ä¼¼åº¦: {result['score']:.3f})")


def example_memory_update():
    """ç¤ºä¾‹2: è®°å¿†æ›´æ–°å’Œå†²çªå¤„ç†"""
    print("\n" + "=" * 70)
    print("ğŸ”„ ç¤ºä¾‹2: è®°å¿†æ›´æ–°")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_update",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # åˆå§‹è®°å¿†
    print("\n1ï¸âƒ£ å†™å…¥åˆå§‹ä¿¡æ¯...")
    memory.write_memory(
        "æˆ‘å«ææ˜ï¼Œä»Šå¹´25å²ï¼Œåœ¨ä¸Šæµ·åšäº§å“ç»ç†",
        user_id="user_li",
        agent_id="assistant"
    )
    print("âœ… åˆå§‹è®°å¿†å·²ä¿å­˜")
    
    # æœç´¢å½“å‰èŒä¸š
    print("\n2ï¸âƒ£ æŸ¥è¯¢å½“å‰èŒä¸š...")
    results = memory.search_memory(
        "ææ˜çš„èŒä¸š",
        user_id="user_li",
        limit=1
    )
    if results:
        print(f"  å½“å‰èŒä¸š: {results[0]['text']}")
    
    # æ›´æ–°ä¿¡æ¯
    print("\n3ï¸âƒ£ æ›´æ–°èŒä¸šä¿¡æ¯...")
    memory.write_memory(
        "æˆ‘ç°åœ¨æ¢å·¥ä½œäº†ï¼Œæˆä¸ºäº†ä¸€åæ•°æ®ç§‘å­¦å®¶",
        user_id="user_li",
        agent_id="assistant"
    )
    print("âœ… è®°å¿†å·²æ›´æ–°")
    
    # å†æ¬¡æœç´¢
    print("\n4ï¸âƒ£ éªŒè¯æ›´æ–°ç»“æœ...")
    results = memory.search_memory(
        "ææ˜çš„èŒä¸š",
        user_id="user_li",
        limit=2
    )
    
    print("  æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"    {i}. {result['text']}")
    
    print("\nâœ… ç¤ºä¾‹2å®Œæˆ")


def example_multi_user():
    """ç¤ºä¾‹3: å¤šç”¨æˆ·è®°å¿†éš”ç¦»"""
    print("\n" + "=" * 70)
    print("ğŸ‘¥ ç¤ºä¾‹3: å¤šç”¨æˆ·åœºæ™¯")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_multiuser",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # ç”¨æˆ·Açš„è®°å¿†
    print("\n1ï¸âƒ£ ç”¨æˆ·Açš„å¯¹è¯...")
    memory.write_memory(
        "æˆ‘å–œæ¬¢åƒå·èœï¼Œç‰¹åˆ«æ˜¯éº»å©†è±†è…",
        user_id="user_a",
        agent_id="assistant"
    )
    
    # ç”¨æˆ·Bçš„è®°å¿†
    print("2ï¸âƒ£ ç”¨æˆ·Bçš„å¯¹è¯...")
    memory.write_memory(
        "æˆ‘å–œæ¬¢åƒç²¤èœï¼Œç‰¹åˆ«æ˜¯ç™½åˆ‡é¸¡",
        user_id="user_b",
        agent_id="assistant"
    )
    
    # åˆ†åˆ«æœç´¢
    print("\n3ï¸âƒ£ åˆ†åˆ«æœç´¢ç”¨æˆ·åå¥½...")
    
    print("  ç”¨æˆ·Açš„æœç´¢ç»“æœ:")
    results_a = memory.search_memory(
        "å–œæ¬¢åƒä»€ä¹ˆèœ",
        user_id="user_a",
        limit=1
    )
    for r in results_a:
        print(f"    - {r['text']}")
    
    print("  ç”¨æˆ·Bçš„æœç´¢ç»“æœ:")
    results_b = memory.search_memory(
        "å–œæ¬¢åƒä»€ä¹ˆèœ",
        user_id="user_b",
        limit=1
    )
    for r in results_b:
        print(f"    - {r['text']}")
    
    print("\nâœ… ç¤ºä¾‹3å®Œæˆ - è®°å¿†å·²æ­£ç¡®éš”ç¦»")


def example_fact_extraction():
    """ç¤ºä¾‹4: äº‹å®æå–åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ç¤ºä¾‹4: äº‹å®æå–")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_facts",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„å¯¹è¯
    test_cases = [
        ("ç®€å•é—®å€™", "ä½ å¥½"),
        ("ä¸ªäººä¿¡æ¯", "æˆ‘å«ç‹èŠ³ï¼Œä»Šå¹´30å²ï¼Œä½åœ¨æ·±åœ³"),
        ("å…´è¶£çˆ±å¥½", "æˆ‘å–œæ¬¢æ—…æ¸¸ï¼Œå»å¹´å»äº†æ—¥æœ¬å’ŒéŸ©å›½"),
        ("å·¥ä½œä¿¡æ¯", "æˆ‘åœ¨ä¸€å®¶äº’è”ç½‘å…¬å¸æ‹…ä»»UIè®¾è®¡å¸ˆ"),
        ("æœªæ¥è®¡åˆ’", "æˆ‘è®¡åˆ’æ˜å¹´å­¦ä¹ æ‘„å½±")
    ]
    
    print("\næµ‹è¯•äº‹å®æå–åŠŸèƒ½:\n")
    for label, conversation in test_cases:
        print(f"  [{label}]")
        print(f"  å¯¹è¯: {conversation}")
        
        facts = memory.extract_facts(conversation)
        
        if facts:
            print(f"  æå–äº‹å®: {facts}")
        else:
            print("  æå–äº‹å®: (æ— å®è´¨æ€§ä¿¡æ¯)")
        print()
    
    print("âœ… ç¤ºä¾‹4å®Œæˆ")


def example_advanced_search():
    """ç¤ºä¾‹5: é«˜çº§æœç´¢åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("ğŸ” ç¤ºä¾‹5: é«˜çº§æœç´¢")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_search",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim
    )
    
    # å‡†å¤‡ä¸°å¯Œçš„è®°å¿†æ•°æ®
    print("\n1ï¸âƒ£ å‡†å¤‡æµ‹è¯•æ•°æ®...")
    knowledge = [
        "æˆ‘åœ¨2020å¹´æ¯•ä¸šäºæ¸…åå¤§å­¦è®¡ç®—æœºç³»",
        "æˆ‘çš„ç¬¬ä¸€ä»½å·¥ä½œæ˜¯åœ¨å­—èŠ‚è·³åŠ¨åšåç«¯å¼€å‘",
        "2022å¹´æˆ‘è·³æ§½åˆ°äº†é˜¿é‡Œå·´å·´",
        "æˆ‘ç°åœ¨è´Ÿè´£ç”µå•†æ¨èç³»ç»Ÿçš„å¼€å‘",
        "æˆ‘æœ€æ“…é•¿çš„æŠ€æœ¯æ ˆæ˜¯Pythonå’ŒGo",
        "ä¸šä½™æ—¶é—´æˆ‘å–œæ¬¢ç ”ç©¶æœºå™¨å­¦ä¹ ç®—æ³•",
        "æˆ‘çš„é•¿æœŸç›®æ ‡æ˜¯æˆä¸ºä¸€åæŠ€æœ¯ä¸“å®¶"
    ]
    
    for k in knowledge:
        memory.write_memory(k, user_id="user_tech", agent_id="assistant")
    
    print(f"âœ… å·²å†™å…¥ {len(knowledge)} æ¡è®°å¿†")
    
    # ä¸åŒç±»å‹çš„æœç´¢
    print("\n2ï¸âƒ£ æ‰§è¡Œä¸åŒç±»å‹çš„æœç´¢...\n")
    
    search_cases = [
        ("æ•™è‚²èƒŒæ™¯", "æ¯•ä¸šé™¢æ ¡"),
        ("å·¥ä½œç»å†", "å·¥ä½œå˜åŠ¨å†å²"),
        ("æŠ€æœ¯èƒ½åŠ›", "æ“…é•¿çš„ç¼–ç¨‹è¯­è¨€"),
        ("å…´è¶£çˆ±å¥½", "ä¸šä½™çˆ±å¥½"),
        ("èŒä¸šè§„åˆ’", "æœªæ¥ç›®æ ‡")
    ]
    
    for category, query in search_cases:
        print(f"  [{category}] æŸ¥è¯¢: {query}")
        results = memory.search_memory(
            query=query,
            user_id="user_tech",
            limit=2
        )
        
        if results:
            for i, r in enumerate(results, 1):
                print(f"    {i}. {r['text']} (åˆ†æ•°: {r['score']:.3f})")
        else:
            print("    æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        print()
    
    print("âœ… ç¤ºä¾‹5å®Œæˆ")


if __name__ == "__main__":
    print("=" * 70)
    print("TinyMem0 è®°å¿†ç³»ç»Ÿå®Œæ•´ç¤ºä¾‹")
    print("é…ç½®æ¥æº: .env æ–‡ä»¶")
    print("=" * 70)
    
    # ä¸‹è½½æ¨¡å‹ï¼ˆæ ¹æ®.envé…ç½®ï¼‰
    downloaded_path = download_models()
    
    # å¦‚æœä¸‹è½½äº†æ¨¡å‹ï¼Œæ›´æ–°.envä¸­çš„LOCAL_MODEL_PATH
    if downloaded_path:
        env_file = Path(__file__).parent.parent / '.env'
        if env_file.exists():
            lines = []
            updated = False
            with open(env_file, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.startswith('LOCAL_MODEL_PATH='):
                        lines.append(f'LOCAL_MODEL_PATH={downloaded_path}\n')
                        updated = True
                    else:
                        lines.append(line)
            
            if updated:
                with open(env_file, 'w', encoding='utf-8') as f:
                    f.writelines(lines)
                print(f"\nâœ… å·²æ›´æ–° .env: LOCAL_MODEL_PATH={downloaded_path}\n")
                # é‡æ–°åŠ è½½.env
                load_dotenv(override=True)
    
    # è¿è¡Œä¸»ç¤ºä¾‹
    main()
