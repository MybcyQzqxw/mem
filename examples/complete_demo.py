#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è®°å¿†ç³»ç»Ÿä½¿ç”¨ç¤ºä¾‹ - è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å¹¶è¿è¡Œ
"""
import sys
import os
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'src'))
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv
load_dotenv()
from tinymem0 import MemorySystem


def download_models(model_shortcut='qwen2.5-7b', model_format='gguf', quantization='Q4_K_M', use_local_llm=True, embedding_model='BAAI/bge-small-zh-v1.5'):
    """è‡ªåŠ¨ä¸‹è½½æ¨¡å‹
    
    Args:
        model_shortcut: æ¨¡å‹å¿«æ·åç§° (qwen2.5-7b, mistral-7bç­‰)
        model_format: æ¨¡å‹æ ¼å¼ (ggufæˆ–safetensors)
        quantization: GGUFé‡åŒ–ç²¾åº¦ (Q4_K_M, Q5_K_Mç­‰ï¼Œä»…ggufæ ¼å¼éœ€è¦)
        use_local_llm: æ˜¯å¦ä½¿ç”¨æœ¬åœ°LLM
        embedding_model: åµŒå…¥æ¨¡å‹åç§°
    """
    print("=" * 70)
    print("ğŸ“¦ æ£€æŸ¥å¹¶ä¸‹è½½æ¨¡å‹")
    print("=" * 70)
    
    # 1. åµŒå…¥æ¨¡å‹ (ç”±MemorySystemè‡ªåŠ¨ç®¡ç†)
    print("\n1ï¸âƒ£ åµŒå…¥æ¨¡å‹...")
    print(f"   æ¨¡å‹: {embedding_model}")
    print("   ğŸ“ ä¿å­˜åˆ°: ./models/embeddings (é¦–æ¬¡ä½¿ç”¨æ—¶è‡ªåŠ¨ä¸‹è½½)")
    
    # 2. æ£€æŸ¥LLMæ¨¡å‹
    print("\n2ï¸âƒ£ LLMæ¨¡å‹...")
    
    if not use_local_llm:
        print("   â­ï¸  äº‘ç«¯APIæ¨¡å¼ï¼Œæ— éœ€ä¸‹è½½")
        print("\n" + "=" * 70)
        return
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    if model_format == 'gguf':
        model_dir = Path('./models/gguf')
        if model_dir.exists():
            gguf_files = list(model_dir.glob('*.gguf'))
            if gguf_files:
                print(f"   âœ… æ¨¡å‹å·²å­˜åœ¨: {gguf_files[0]}")
                print("\n" + "=" * 70)
                return
    else:
        model_dir = Path('./models/safetensors') / model_shortcut
        if model_dir.exists() and list(model_dir.glob('*')):
            print(f"   âœ… æ¨¡å‹å·²å­˜åœ¨: {model_dir}")
            print("\n" + "=" * 70)
            return
    
    # æ¨¡å‹ä¸å­˜åœ¨ï¼Œè°ƒç”¨ä¸‹è½½å·¥å…·
    print(f"   âŒ æ¨¡å‹ä¸å­˜åœ¨ï¼Œå‡†å¤‡ä¸‹è½½...\n")
    
    # æ·»åŠ scriptsåˆ°è·¯å¾„å¹¶è°ƒç”¨ä¸‹è½½å‡½æ•°
    sys.path.insert(0, str(Path(__file__).parent.parent / 'scripts'))
    from download_llm import download_model_with_shortcut
    
    try:
        downloaded_path = download_model_with_shortcut(
            model_shortcut=model_shortcut,
            model_format=model_format,
            quantization=quantization,
            verbose=True
        )
        
        print(f"\n   âœ… æ¨¡å‹ä¸‹è½½å®Œæˆï¼")
        print(f"   ğŸ“‚ ä½ç½®: {downloaded_path}")
        
        # æ›´æ–° .env æ–‡ä»¶ä¸­çš„æ¨¡å‹è·¯å¾„
        env_file = Path(__file__).parent.parent / '.env'
        if env_file.exists():
            with open(env_file, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            
            with open(env_file, 'w', encoding='utf-8') as f:
                for line in lines:
                    if line.startswith('LOCAL_MODEL_PATH='):
                        f.write(f'LOCAL_MODEL_PATH={downloaded_path}\n')
                        print(f"   ğŸ”§ å·²æ›´æ–° .env: LOCAL_MODEL_PATH={downloaded_path}")
                    else:
                        f.write(line)
        
    except Exception as e:
        print(f"\n   âŒ ä¸‹è½½å¤±è´¥: {e}")
        print(f"   ğŸ’¡ ä½ å¯ä»¥æ‰‹åŠ¨è¿è¡Œ:")
        print(f"   python scripts/download_llm.py --model {model_shortcut} --format {model_format}")
    
    print("\n" + "=" * 70)


def main(use_local_llm=None, local_model_path=None, local_embedding_model=None, embedding_dim=None, embedding_cache_dir=None):
    """ä¸»å‡½æ•° - æ¼”ç¤ºè®°å¿†ç³»ç»Ÿçš„ä½¿ç”¨
    
    Args:
        use_local_llm: æ˜¯å¦ä½¿ç”¨æœ¬åœ°LLM
        local_model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
        local_embedding_model: æœ¬åœ°åµŒå…¥æ¨¡å‹
        embedding_dim: åµŒå…¥å‘é‡ç»´åº¦
        embedding_cache_dir: åµŒå…¥æ¨¡å‹ç¼“å­˜ç›®å½•
    """
    import os
    # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°ï¼Œä¸å†è¯»å–.env
    use_local = use_local_llm if use_local_llm is not None else False
    
    if not use_local and not os.getenv("DASHSCOPE_API_KEY"):
        raise RuntimeError("æœªæ‰¾åˆ° DASHSCOPE_API_KEYï¼Œè¯·åœ¨ .env ä¸­é…ç½®ã€‚")
    
    mode = "æœ¬åœ°æ¨¡å‹" if use_local else "äº‘ç«¯API"
    if use_local and local_model_path:
        print(f"åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ ({mode}: {local_model_path})...")
    else:
        print(f"åˆå§‹åŒ–è®°å¿†ç³»ç»Ÿ ({mode})...")
    
    memory_system = MemorySystem(
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim,
        embedding_cache_dir=embedding_cache_dir
    )
    
    # ç¤ºä¾‹1: å†™å…¥è®°å¿†
    print("\n=== ç¤ºä¾‹1: å†™å…¥è®°å¿† ===")
    conversation1 = "ä½ å¥½ï¼Œæˆ‘å«å¼ ä¸‰ï¼Œæ˜¯ä¸€åè½¯ä»¶å·¥ç¨‹å¸ˆï¼Œæˆ‘å–œæ¬¢çœ‹ç”µå½±ï¼Œç‰¹åˆ«æ˜¯ç§‘å¹»ç‰‡ã€‚"
    print(f"ç”¨æˆ·å¯¹è¯: {conversation1}")
    
    memory_system.write_memory(
        conversation=conversation1,
        user_id="user_001",
        agent_id="agent_001"
    )
    
    # ç¤ºä¾‹2: æœç´¢è®°å¿†
    print("\n=== ç¤ºä¾‹2: æœç´¢è®°å¿† ===")
    query = "å¼ ä¸‰çš„èŒä¸šæ˜¯ä»€ä¹ˆï¼Ÿ"
    print(f"æœç´¢æŸ¥è¯¢: {query}")
    
    results = memory_system.search_memory(
        query=query,
        user_id="user_001",
        agent_id="agent_001",
        limit=3
    )
    
    print("æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['text']} (ç›¸ä¼¼åº¦: {result['score']:.3f})")
    
    # ç¤ºä¾‹3: æ›´æ–°è®°å¿†
    print("\n=== ç¤ºä¾‹3: æ›´æ–°è®°å¿† ===")
    conversation2 = "æˆ‘æœ€è¿‘æ”¹å˜äº†èŒä¸šï¼Œç°åœ¨æ˜¯ä¸€åäº§å“ç»ç†ï¼Œä¸å†åšè½¯ä»¶å·¥ç¨‹å¸ˆäº†ã€‚"
    print(f"ç”¨æˆ·å¯¹è¯: {conversation2}")
    
    memory_system.write_memory(
        conversation=conversation2,
        user_id="user_001",
        agent_id="agent_001"
    )
    
    # å†æ¬¡æœç´¢éªŒè¯æ›´æ–°
    print("\næ›´æ–°åå†æ¬¡æœç´¢:")
    results = memory_system.search_memory(
        query="å¼ ä¸‰çš„èŒä¸š",
        user_id="user_001",
        agent_id="agent_001",
        limit=3
    )
    
    print("æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"{i}. {result['text']} (ç›¸ä¼¼åº¦: {result['score']:.3f})")


def example_memory_update():
    """ç¤ºä¾‹2: è®°å¿†æ›´æ–°å’Œå†²çªå¤„ç†"""
    print("\n" + "=" * 70)
    print("ğŸ”„ ç¤ºä¾‹2: è®°å¿†æ›´æ–°")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_update",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim,
        embedding_cache_dir=embedding_cache_dir
    )
    
    # åˆå§‹è®°å¿†
    print("\n1ï¸âƒ£ å†™å…¥åˆå§‹ä¿¡æ¯...")
    memory.write_memory(
        "æˆ‘å«ææ˜ï¼Œä»Šå¹´25å²ï¼Œåœ¨ä¸Šæµ·åšäº§å“ç»ç†",
        user_id="user_li",
        agent_id="assistant"
    )
    print("âœ… åˆå§‹è®°å¿†å·²ä¿å­˜")
    
    # æœç´¢å½“å‰èŒä¸š
    print("\n2ï¸âƒ£ æŸ¥è¯¢å½“å‰èŒä¸š...")
    results = memory.search_memory(
        "ææ˜çš„èŒä¸š",
        user_id="user_li",
        limit=1
    )
    if results:
        print(f"  å½“å‰èŒä¸š: {results[0]['text']}")
    
    # æ›´æ–°ä¿¡æ¯
    print("\n3ï¸âƒ£ æ›´æ–°èŒä¸šä¿¡æ¯...")
    memory.write_memory(
        "æˆ‘ç°åœ¨æ¢å·¥ä½œäº†ï¼Œæˆä¸ºäº†ä¸€åæ•°æ®ç§‘å­¦å®¶",
        user_id="user_li",
        agent_id="assistant"
    )
    print("âœ… è®°å¿†å·²æ›´æ–°")
    
    # å†æ¬¡æœç´¢
    print("\n4ï¸âƒ£ éªŒè¯æ›´æ–°ç»“æœ...")
    results = memory.search_memory(
        "ææ˜çš„èŒä¸š",
        user_id="user_li",
        limit=2
    )
    
    print("  æœç´¢ç»“æœ:")
    for i, result in enumerate(results, 1):
        print(f"    {i}. {result['text']}")
    
    print("\nâœ… ç¤ºä¾‹2å®Œæˆ")


def example_multi_user():
    """ç¤ºä¾‹3: å¤šç”¨æˆ·è®°å¿†éš”ç¦»"""
    print("\n" + "=" * 70)
    print("ğŸ‘¥ ç¤ºä¾‹3: å¤šç”¨æˆ·åœºæ™¯")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_multiuser",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim,
        embedding_cache_dir=embedding_cache_dir
    )
    
    # ç”¨æˆ·Açš„è®°å¿†
    print("\n1ï¸âƒ£ ç”¨æˆ·Açš„å¯¹è¯...")
    memory.write_memory(
        "æˆ‘å–œæ¬¢åƒå·èœï¼Œç‰¹åˆ«æ˜¯éº»å©†è±†è…",
        user_id="user_a",
        agent_id="assistant"
    )
    
    # ç”¨æˆ·Bçš„è®°å¿†
    print("2ï¸âƒ£ ç”¨æˆ·Bçš„å¯¹è¯...")
    memory.write_memory(
        "æˆ‘å–œæ¬¢åƒç²¤èœï¼Œç‰¹åˆ«æ˜¯ç™½åˆ‡é¸¡",
        user_id="user_b",
        agent_id="assistant"
    )
    
    # åˆ†åˆ«æœç´¢
    print("\n3ï¸âƒ£ åˆ†åˆ«æœç´¢ç”¨æˆ·åå¥½...")
    
    print("  ç”¨æˆ·Açš„æœç´¢ç»“æœ:")
    results_a = memory.search_memory(
        "å–œæ¬¢åƒä»€ä¹ˆèœ",
        user_id="user_a",
        limit=1
    )
    for r in results_a:
        print(f"    - {r['text']}")
    
    print("  ç”¨æˆ·Bçš„æœç´¢ç»“æœ:")
    results_b = memory.search_memory(
        "å–œæ¬¢åƒä»€ä¹ˆèœ",
        user_id="user_b",
        limit=1
    )
    for r in results_b:
        print(f"    - {r['text']}")
    
    print("\nâœ… ç¤ºä¾‹3å®Œæˆ - è®°å¿†å·²æ­£ç¡®éš”ç¦»")


def example_fact_extraction():
    """ç¤ºä¾‹4: äº‹å®æå–åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("ğŸ“Š ç¤ºä¾‹4: äº‹å®æå–")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_facts",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim,
        embedding_cache_dir=embedding_cache_dir
    )
    
    # æµ‹è¯•ä¸åŒç±»å‹çš„å¯¹è¯
    test_cases = [
        ("ç®€å•é—®å€™", "ä½ å¥½"),
        ("ä¸ªäººä¿¡æ¯", "æˆ‘å«ç‹èŠ³ï¼Œä»Šå¹´30å²ï¼Œä½åœ¨æ·±åœ³"),
        ("å…´è¶£çˆ±å¥½", "æˆ‘å–œæ¬¢æ—…æ¸¸ï¼Œå»å¹´å»äº†æ—¥æœ¬å’ŒéŸ©å›½"),
        ("å·¥ä½œä¿¡æ¯", "æˆ‘åœ¨ä¸€å®¶äº’è”ç½‘å…¬å¸æ‹…ä»»UIè®¾è®¡å¸ˆ"),
        ("æœªæ¥è®¡åˆ’", "æˆ‘è®¡åˆ’æ˜å¹´å­¦ä¹ æ‘„å½±")
    ]
    
    print("\næµ‹è¯•äº‹å®æå–åŠŸèƒ½:\n")
    for label, conversation in test_cases:
        print(f"  [{label}]")
        print(f"  å¯¹è¯: {conversation}")
        
        facts = memory.extract_facts(conversation)
        
        if facts:
            print(f"  æå–äº‹å®: {facts}")
        else:
            print("  æå–äº‹å®: (æ— å®è´¨æ€§ä¿¡æ¯)")
        print()
    
    print("âœ… ç¤ºä¾‹4å®Œæˆ")


def example_advanced_search():
    """ç¤ºä¾‹5: é«˜çº§æœç´¢åŠŸèƒ½"""
    print("\n" + "=" * 70)
    print("ğŸ” ç¤ºä¾‹5: é«˜çº§æœç´¢")
    print("=" * 70)
    
    memory = MemorySystem(
        collection_name="demo_search",
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim,
        embedding_cache_dir=embedding_cache_dir
    )
    
    # å‡†å¤‡ä¸°å¯Œçš„è®°å¿†æ•°æ®
    print("\n1ï¸âƒ£ å‡†å¤‡æµ‹è¯•æ•°æ®...")
    knowledge = [
        "æˆ‘åœ¨2020å¹´æ¯•ä¸šäºæ¸…åå¤§å­¦è®¡ç®—æœºç³»",
        "æˆ‘çš„ç¬¬ä¸€ä»½å·¥ä½œæ˜¯åœ¨å­—èŠ‚è·³åŠ¨åšåç«¯å¼€å‘",
        "2022å¹´æˆ‘è·³æ§½åˆ°äº†é˜¿é‡Œå·´å·´",
        "æˆ‘ç°åœ¨è´Ÿè´£ç”µå•†æ¨èç³»ç»Ÿçš„å¼€å‘",
        "æˆ‘æœ€æ“…é•¿çš„æŠ€æœ¯æ ˆæ˜¯Pythonå’ŒGo",
        "ä¸šä½™æ—¶é—´æˆ‘å–œæ¬¢ç ”ç©¶æœºå™¨å­¦ä¹ ç®—æ³•",
        "æˆ‘çš„é•¿æœŸç›®æ ‡æ˜¯æˆä¸ºä¸€åæŠ€æœ¯ä¸“å®¶"
    ]
    
    for k in knowledge:
        memory.write_memory(k, user_id="user_tech", agent_id="assistant")
    
    print(f"âœ… å·²å†™å…¥ {len(knowledge)} æ¡è®°å¿†")
    
    # ä¸åŒç±»å‹çš„æœç´¢
    print("\n2ï¸âƒ£ æ‰§è¡Œä¸åŒç±»å‹çš„æœç´¢...\n")
    
    search_cases = [
        ("æ•™è‚²èƒŒæ™¯", "æ¯•ä¸šé™¢æ ¡"),
        ("å·¥ä½œç»å†", "å·¥ä½œå˜åŠ¨å†å²"),
        ("æŠ€æœ¯èƒ½åŠ›", "æ“…é•¿çš„ç¼–ç¨‹è¯­è¨€"),
        ("å…´è¶£çˆ±å¥½", "ä¸šä½™çˆ±å¥½"),
        ("èŒä¸šè§„åˆ’", "æœªæ¥ç›®æ ‡")
    ]
    
    for category, query in search_cases:
        print(f"  [{category}] æŸ¥è¯¢: {query}")
        results = memory.search_memory(
            query=query,
            user_id="user_tech",
            limit=2
        )
        
        if results:
            for i, r in enumerate(results, 1):
                print(f"    {i}. {r['text']} (åˆ†æ•°: {r['score']:.3f})")
        else:
            print("    æœªæ‰¾åˆ°ç›¸å…³ç»“æœ")
        print()
    
    print("âœ… ç¤ºä¾‹5å®Œæˆ")


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(
        description='TinyMem0 è®°å¿†ç³»ç»Ÿå®Œæ•´ç¤ºä¾‹ - è‡ªåŠ¨ä¸‹è½½æ¨¡å‹å¹¶è¿è¡Œ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog='''
ç¤ºä¾‹ç”¨æ³•:
  # ä½¿ç”¨é»˜è®¤æ¨¡å‹ (Qwen2.5-7B GGUFæ ¼å¼)
  python examples/complete_demo.py
  
  # æŒ‡å®šå…¶ä»–æ¨¡å‹
  python examples/complete_demo.py --model mistral-7b --format gguf
  python examples/complete_demo.py --model qwen2.5-3b --format safetensors
  
  # è·³è¿‡æ¨¡å‹ä¸‹è½½ï¼ˆä½¿ç”¨äº‘ç«¯APIæˆ–å·²æœ‰æ¨¡å‹ï¼‰
  python examples/complete_demo.py --skip-download

æ”¯æŒçš„æ¨¡å‹:
  qwen2.5-7b, qwen2.5-3b, qwen2.5-1.5b
  mistral-7b, llama3-8b, yi-6b

æ”¯æŒçš„æ ¼å¼:
  gguf       - CPUæ¨ç†ï¼Œ4-8GB (æ¨è)
  safetensors - GPUæ¨ç†ï¼Œ14-26GB
        ''')
    
    parser.add_argument(
        '--model', '-m',
        type=str,
        default='mistral-7b',
        choices=['qwen2.5-7b', 'qwen2.5-3b', 'qwen2.5-1.5b',
                'mistral-7b', 'llama3-8b', 'yi-6b'],
        help='é€‰æ‹©æ¨¡å‹ (é»˜è®¤: mistral-7b, ä½¿ç”¨TheBloke/Mistral-7B-Instruct-v0.2-GGUF)'
    )
    
    parser.add_argument(
        '--format', '-f',
        type=str,
        default='gguf',
        choices=['gguf', 'safetensors'],
        help='æ¨¡å‹æ ¼å¼ (é»˜è®¤: gguf)'
    )
    
    parser.add_argument(
        '--quant', '-q',
        type=str,
        default='Q4_K_M',
        choices=['Q3_K_M', 'Q4_K_M', 'Q5_K_M', 'Q8_0'],
        help='GGUFé‡åŒ–ç²¾åº¦ (é»˜è®¤: Q4_K_M, ä»…format=ggufæ—¶æœ‰æ•ˆ)'
    )
    
    parser.add_argument(
        '--use-local',
        action='store_true',
        help='ä½¿ç”¨æœ¬åœ°LLMï¼ˆä¼˜å…ˆçº§é«˜äº.envï¼‰'
    )
    
    parser.add_argument(
        '--use-cloud',
        action='store_true',
        help='ä½¿ç”¨äº‘ç«¯APIï¼ˆè¦†ç›–--use-localå’Œ.envï¼‰'
    )
    
    parser.add_argument(
        '--embedding-model',
        type=str,
        help='åµŒå…¥æ¨¡å‹åç§° (é»˜è®¤: BAAI/bge-small-zh-v1.5)'
    )
    
    parser.add_argument(
        '--embedding-dim',
        type=int,
        help='åµŒå…¥å‘é‡ç»´åº¦ (é»˜è®¤: æœ¬åœ°512/äº‘ç«¯1536)'
    )
    
    parser.add_argument(
        '--embedding-cache-dir',
        type=str,
        default='./models/embeddings',
        help='åµŒå…¥æ¨¡å‹ç¼“å­˜ç›®å½• (é»˜è®¤: ./models/embeddings)'
    )
    
    parser.add_argument(
        '--skip-download', '-s',
        action='store_true',
        help='è·³è¿‡æ¨¡å‹ä¸‹è½½ï¼Œç›´æ¥è¿è¡Œdemo'
    )
    
    args = parser.parse_args()
    
    # ç¡®å®šæ˜¯å¦ä½¿ç”¨æœ¬åœ°LLMï¼š--use-cloud > --use-local > é»˜è®¤False
    if args.use_cloud:
        use_local = False
    elif args.use_local:
        use_local = True
    else:
        use_local = False  # é»˜è®¤ä½¿ç”¨äº‘ç«¯API
    
    local_model_path = None
    local_embedding_model = args.embedding_model or "BAAI/bge-small-zh-v1.5"
    embedding_dim = args.embedding_dim
    
    # ä¸‹è½½æ¨¡å‹(é™¤éæ˜ç¡®è·³è¿‡)
    if not args.skip_download and use_local:
        download_models(
            model_shortcut=args.model,
            model_format=args.format,
            quantization=args.quant,
            use_local_llm=use_local,
            embedding_model=local_embedding_model
        )
        
        # æ„å»ºæ¨¡å‹è·¯å¾„
        if args.format == 'gguf':
            # GGUFæ–‡ä»¶ç›´æ¥åœ¨models/ggufç›®å½•ä¸‹
            model_dir = Path('./models/gguf')
            if model_dir.exists():
                # æŸ¥æ‰¾åŒ¹é…é‡åŒ–ç²¾åº¦çš„æ–‡ä»¶
                pattern = f'*{args.quant}*.gguf'
                gguf_files = list(model_dir.glob(pattern))
                if gguf_files:
                    local_model_path = str(gguf_files[0])
        else:
            # SafeTensorsåœ¨å­ç›®å½•
            model_dir = Path('./models/safetensors') / args.model
            if model_dir.exists():
                local_model_path = str(model_dir)
        
        print("\n")
    else:
        if args.skip_download:
            print("â­ï¸  è·³è¿‡æ¨¡å‹ä¸‹è½½\n")
        # å³ä½¿è·³è¿‡ä¸‹è½½ï¼Œä¹Ÿå°è¯•æ„å»ºè·¯å¾„
        if use_local and args.format == 'gguf':
            model_dir = Path('./models/gguf')
            if model_dir.exists():
                pattern = f'*{args.quant}*.gguf'
                gguf_files = list(model_dir.glob(pattern))
                if gguf_files:
                    local_model_path = str(gguf_files[0])
    
    # è¿è¡Œä¸»ç¤ºä¾‹ï¼Œä¼ é€’å‚æ•°
    main(
        use_local_llm=use_local,
        local_model_path=local_model_path,
        local_embedding_model=local_embedding_model,
        embedding_dim=embedding_dim,
        embedding_cache_dir=args.embedding_cache_dir
    )
