#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LLM æ¨¡å‹é…ç½®åŠ©æ‰‹

è¯¥è„šæœ¬ç”¨äºå¸®åŠ©ç”¨æˆ·é…ç½®æœ¬åœ°LLMæ¨¡å‹ã€‚
æ”¯æŒGGUFå’ŒSafeTensorsä¸¤ç§æ ¼å¼ã€‚
æ³¨æ„ï¼šæ­¤è„šæœ¬ä¸ä¼šä¸‹è½½æ¨¡å‹ï¼Œç”¨æˆ·éœ€è¦æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶ã€‚

åŠŸèƒ½ï¼š
1. è‡ªåŠ¨åˆ›å»º ./models/gguf å’Œ ./models/safetensors ç›®å½•
2. æ£€æµ‹ä¸¤ç§æ ¼å¼çš„æ¨¡å‹æ–‡ä»¶
3. å°†é€‰å®šçš„æ¨¡å‹è·¯å¾„å†™å…¥ .env æ–‡ä»¶

ä½¿ç”¨æ–¹æ³•ï¼š
    python scripts/setup_llm.py

æ¨èçš„æ¨¡å‹ä¸‹è½½æºï¼š
- GGUFæ ¼å¼: https://huggingface.co/models?library=gguf
- SafeTensors: https://huggingface.co/models?library=transformers
- ModelScope: https://modelscope.cn/models
"""

import sys
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.model_manager import (
    ensure_models_directory,
    find_gguf_models,
    find_safetensors_models,
    list_available_models,
    configure_llm_model,
    get_model_info,
    validate_model,
    GGUF_DIR,
    SAFETENSORS_DIR
)


def print_banner():
    """æ‰“å°æ¬¢è¿ä¿¡æ¯"""
    print("=" * 70)
    print("ğŸ”§ TinyMem0 LLM æ¨¡å‹é…ç½®åŠ©æ‰‹")
    print("=" * 70)
    print()
    print("ğŸ“ è¯´æ˜ï¼š")
    print("  æ­¤å·¥å…·ç”¨äºé…ç½®æœ¬åœ°LLMæ¨¡å‹ï¼ˆGGUF æˆ– SafeTensorsæ ¼å¼ï¼‰")
    print("  ä¸ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œè¯·å…ˆæ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
    print()
    print("ğŸ“ ç›®å½•ç»“æ„ï¼š")
    print(f"  - GGUFæ¨¡å‹: {GGUF_DIR}/")
    print(f"  - SafeTensorsæ¨¡å‹: {SAFETENSORS_DIR}/")
    print()
    print("ğŸŒ æ¨èä¸‹è½½æºï¼š")
    print("  - GGUF: https://huggingface.co/models?library=gguf")
    print("  - SafeTensors: https://huggingface.co/models?library=transformers")
    print("  - ModelScope: https://modelscope.cn/models")
    print()


def print_download_guide():
    """æ‰“å°ä¸‹è½½æŒ‡å—"""
    print("ğŸ“š æ¨¡å‹ä¸‹è½½æŒ‡å—ï¼š")
    print()
    print("==== GGUF æ ¼å¼ï¼ˆé‡åŒ–ï¼Œæ¨èä½é…ç½®æœºå™¨ï¼‰ ====")
    print("æ¨èæ¨¡å‹ï¼š")
    print("  1. Qwen2-7B-Instruct (Q4_K_M) - ä¸­æ–‡ä¼˜åŒ–ï¼Œ4GB")
    print("  2. Mistral-7B-Instruct (Q4_K_M) - é€šç”¨æ€§å¥½ï¼Œ4GB")
    print("  3. Llama-3-8B-Instruct (Q4_K_M) - Meta å®˜æ–¹ï¼Œ5GB")
    print()
    print("ä¸‹è½½æ­¥éª¤ï¼š")
    print("  1. è®¿é—® https://huggingface.co/models?library=gguf")
    print("  2. æœç´¢æ¨¡å‹åç§° + 'GGUF'")
    print("  3. ä¸‹è½½ Q4_K_M æˆ– Q5_K_M é‡åŒ–ç‰ˆæœ¬")
    print("  4. å°† .gguf æ–‡ä»¶æ”¾å…¥ ./models/gguf/ ç›®å½•")
    print("  5. é‡æ–°è¿è¡Œæ­¤è„šæœ¬")
    print()
    print("==== SafeTensors æ ¼å¼ï¼ˆåŸå§‹ç²¾åº¦ï¼Œéœ€é«˜é…ç½®ï¼‰ ====")
    print("æ¨èæ¨¡å‹ï¼ˆéœ€è¦12GB+æ˜¾å­˜ï¼‰ï¼š")
    print("  1. Qwen/Qwen2-7B-Instruct")
    print("  2. mistralai/Mistral-7B-Instruct-v0.2")
    print()
    print("ä¸‹è½½æ­¥éª¤ï¼š")
    print("  pip install huggingface_hub")
    print("  huggingface-cli download MODEL_ID \\")
    print("    --local-dir ./models/safetensors/MODEL_NAME")
    print()


def main():
    """ä¸»å‡½æ•°"""
    print_banner()
    
    # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
    project_root = Path(__file__).parent.parent
    ensure_models_directory(str(project_root / 'models'), create_subdirs=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰æ ¼å¼çš„æ¨¡å‹
    gguf_models = find_gguf_models(str(project_root / 'models' / 'gguf'))
    safetensors_models = find_safetensors_models(
        str(project_root / 'models' / 'safetensors')
    )
    
    all_models = []
    model_types = []
    
    for model in gguf_models:
        all_models.append(model)
        model_types.append('gguf')
    
    for model in safetensors_models:
        all_models.append(model)
        model_types.append('safetensors')
    
    if not all_models:
        print("âš ï¸  æœªå‘ç°ä»»ä½•æ¨¡å‹æ–‡ä»¶")
        print()
        print_download_guide()
        print("=" * 70)
        return
    
    # æ˜¾ç¤ºæ‰¾åˆ°çš„æ¨¡å‹
    print(f"âœ… å‘ç° {len(all_models)} ä¸ªæ¨¡å‹ï¼š")
    print()
    
    for i, model_path in enumerate(all_models, start=1):
        info = get_model_info(model_path, model_types[i-1])
        format_tag = "ğŸ”§GGUF" if model_types[i-1] == 'gguf' else "ğŸ“¦SafeTensors"
        print(f"  [{i}] {format_tag} {info['name']}")
        print(f"      å¤§å°: {info['size_str']}")
        print(f"      è·¯å¾„: {info['path']}")
        print()
    
    # é€‰æ‹©æ¨¡å‹
    choice = None
    if len(all_models) == 1:
        # åªæœ‰ä¸€ä¸ªæ¨¡å‹ï¼Œè‡ªåŠ¨é€‰æ‹©
        choice = 1
        print(f"ğŸ¯ è‡ªåŠ¨é€‰æ‹©å”¯ä¸€çš„æ¨¡å‹: {all_models[0].name}")
        print()
    else:
        # å¤šä¸ªæ¨¡å‹ï¼Œè®©ç”¨æˆ·é€‰æ‹©
        try:
            user_input = input(
                f"è¯·è¾“å…¥è¦é…ç½®çš„æ¨¡å‹åºå· [1-{len(all_models)}] (æŒ‰å›è½¦å–æ¶ˆ): "
            ).strip()
            
            if not user_input:
                print("âŒ å·²å–æ¶ˆé…ç½®")
                return
            
            choice = int(user_input)
            
            if choice < 1 or choice > len(all_models):
                print(f"âŒ æ— æ•ˆçš„åºå·ï¼Œè¯·è¾“å…¥ 1-{len(all_models)}")
                return
                
        except ValueError:
            print("âŒ æ— æ•ˆçš„è¾“å…¥ï¼Œè¯·è¾“å…¥æ•°å­—")
            return
    
    # é…ç½®é€‰å®šçš„æ¨¡å‹
    selected_model = all_models[choice - 1]
    selected_format = model_types[choice - 1]
    env_file = project_root / '.env'
    
    print("ğŸ“ æ­£åœ¨é…ç½®æ¨¡å‹...")
    success = configure_llm_model(
        str(selected_model),
        str(env_file)
    )
    
    if success:
        print()
        print("=" * 70)
        print("âœ… é…ç½®å®Œæˆï¼")
        print()
        print(f"ğŸ“¦ æ¨¡å‹æ ¼å¼: {selected_format}")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {selected_model}")
        print()
        print("ğŸ“‹ ä¸‹ä¸€æ­¥ï¼š")
        print("  1. ç¡®ä¿ .env æ–‡ä»¶ä¸­ USE_LOCAL_LLM=true")
        if selected_format == 'transformers':
            print("  2. ç¡®ä¿å·²å®‰è£…: pip install transformers torch")
        print("  2. è¿è¡Œä½ çš„åº”ç”¨ç¨‹åº")
        print()
        print("ğŸ’¡ æç¤ºï¼š")
        print("  å¦‚éœ€æ›´æ¢æ¨¡å‹ï¼Œé‡æ–°è¿è¡Œ: python scripts/setup_llm.py")
        print("=" * 70)
    else:
        print()
        print("âŒ é…ç½®å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶æƒé™")


if __name__ == '__main__':
    main()
