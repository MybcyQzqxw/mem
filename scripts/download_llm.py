#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
LLMæ¨¡å‹ä¸‹è½½è„šæœ¬
æä¾›äº¤äº’å¼å’Œå‘½ä»¤è¡Œä¸¤ç§æ–¹å¼ä¸‹è½½LLMæ¨¡å‹
æ”¯æŒGGUFå’ŒSafeTensorsä¸¤ç§æ ¼å¼
"""

import sys
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.model_manager import download_llm_model


# æ¨èçš„GGUFæ¨¡å‹åˆ—è¡¨
GGUF_MODELS = {
    '1': {
        'id': 'bartowski/Qwen2.5-7B-Instruct-GGUF',
        'name': 'Qwen2.5-7B-Instruct',
        'short': 'qwen2.5-7b',
        'quant': 'Q4_K_M',
        'size': '~4.4GB',
        'lang': 'ä¸­æ–‡ä¼˜åŒ–',
        'description': 'æ¨èï¼šé˜¿é‡Œäº‘é€šä¹‰åƒé—®2.5ä»£ï¼Œä¸­æ–‡æ•ˆæœä¼˜ç§€'
    },
    '2': {
        'id': 'TheBloke/Mistral-7B-Instruct-v0.2-GGUF',
        'name': 'Mistral-7B-Instruct-v0.2',
        'short': 'mistral-7b',
        'quant': 'Q4_K_M',
        'size': '~4.1GB',
        'lang': 'å¤šè¯­è¨€',
        'description': 'Mistral AIå®˜æ–¹ï¼Œé€šç”¨æ€§å¥½ï¼ŒæŒ‡ä»¤éµå¾ªå¼º'
    },
    '3': {
        'id': 'TheBloke/Meta-Llama-3-8B-Instruct-GGUF',
        'name': 'Llama-3-8B-Instruct',
        'short': 'llama3-8b',
        'quant': 'Q4_K_M',
        'size': '~4.7GB',
        'lang': 'å¤šè¯­è¨€',
        'description': 'Metaå®˜æ–¹Llama 3ï¼Œæ€§èƒ½å¼ºåŠ²'
    },
    '4': {
        'id': 'TheBloke/Yi-6B-Chat-GGUF',
        'name': 'Yi-6B-Chat',
        'short': 'yi-6b',
        'quant': 'Q4_K_M',
        'size': '~3.5GB',
        'lang': 'ä¸­è‹±åŒè¯­',
        'description': 'é›¶ä¸€ä¸‡ç‰©ï¼Œä¸­è‹±åŒè¯­èƒ½åŠ›å‡è¡¡'
    }
}

# æ¨èçš„SafeTensorsæ¨¡å‹åˆ—è¡¨
SAFETENSORS_MODELS = {
    '1': {
        'id': 'Qwen/Qwen2.5-7B-Instruct',
        'name': 'Qwen2.5-7B-Instruct',
        'short': 'qwen2.5-7b',
        'size': '~15GB',
        'lang': 'ä¸­æ–‡ä¼˜åŒ–',
        'description': 'æ¨èï¼šåŸå§‹ç²¾åº¦ï¼Œæœ€ä½³ä¸­æ–‡æ•ˆæœï¼Œéœ€12GB+æ˜¾å­˜'
    },
    '2': {
        'id': 'mistralai/Mistral-7B-Instruct-v0.2',
        'name': 'Mistral-7B-Instruct-v0.2',
        'short': 'mistral-7b',
        'size': '~14GB',
        'lang': 'å¤šè¯­è¨€',
        'description': 'Mistralå®˜æ–¹ï¼Œéœ€12GB+æ˜¾å­˜'
    },
    '3': {
        'id': 'meta-llama/Meta-Llama-3-8B-Instruct',
        'name': 'Llama-3-8B-Instruct',
        'short': 'llama3-8b',
        'size': '~16GB',
        'lang': 'å¤šè¯­è¨€',
        'description': 'Metaå®˜æ–¹ï¼Œéœ€14GB+æ˜¾å­˜ï¼ˆéœ€ç”³è¯·è®¿é—®æƒé™ï¼‰'
    }
}

# ç®€ç§°åˆ°æ¨¡å‹çš„æ˜ å°„
MODEL_SHORTCUTS = {}
for models in [GGUF_MODELS, SAFETENSORS_MODELS]:
    for key, info in models.items():
        if 'short' in info:
            MODEL_SHORTCUTS[info['short']] = {
                'id': info['id'],
                'name': info['name']
            }


def print_banner():
    """æ‰“å°æ¬¢è¿æ¨ªå¹…"""
    print("=" * 70)
    print("ğŸš€ TinyMem0 LLMæ¨¡å‹ä¸‹è½½å·¥å…·")
    print("=" * 70)
    print()


def print_format_choice():
    """æ‰“å°æ ¼å¼é€‰æ‹©èœå•"""
    print("ğŸ“‹ è¯·é€‰æ‹©æ¨¡å‹æ ¼å¼ï¼š\n")
    print("[1] GGUF æ ¼å¼ï¼ˆé‡åŒ–ï¼Œæ¨èï¼‰")
    print("    ä¼˜ç‚¹: å†…å­˜å ç”¨ä½(4-8GB)ï¼ŒCPUå¯è¿è¡Œï¼Œé€Ÿåº¦å¿«")
    print("    é€‚åˆ: ä½é…æœºå™¨ï¼Œæ— GPUæˆ–æ˜¾å­˜ä¸è¶³")
    print()
    print("[2] SafeTensors æ ¼å¼ï¼ˆåŸå§‹ç²¾åº¦ï¼‰")
    print("    ä¼˜ç‚¹: ç²¾åº¦æœ€é«˜ï¼Œæ— æŸé‡åŒ–")
    print("    é€‚åˆ: é«˜é…GPU(12GB+æ˜¾å­˜)ï¼Œè¿½æ±‚æœ€ä½³æ•ˆæœ")
    print()


def print_gguf_models():
    """æ‰“å°GGUFæ¨¡å‹åˆ—è¡¨"""
    print("ğŸ“‹ å¯ç”¨çš„ GGUF æ¨¡å‹ï¼ˆé‡åŒ–ç‰ˆæœ¬: Q4_K_Mï¼‰ï¼š\n")
    for key, model in GGUF_MODELS.items():
        print(f"[{key}] {model['name']}")
        print(f"    æ¨¡å‹ID: {model['id']}")
        print(f"    å¤§å°: {model['size']} | è¯­è¨€: {model['lang']}")
        print(f"    é‡åŒ–: {model['quant']}")
        print(f"    è¯´æ˜: {model['description']}")
        print()


def print_safetensors_models():
    """æ‰“å°SafeTensorsæ¨¡å‹åˆ—è¡¨"""
    print("ğŸ“‹ å¯ç”¨çš„ SafeTensors æ¨¡å‹ï¼ˆFP16åŸå§‹ç²¾åº¦ï¼‰ï¼š\n")
    for key, model in SAFETENSORS_MODELS.items():
        print(f"[{key}] {model['name']}")
        print(f"    æ¨¡å‹ID: {model['id']}")
        print(f"    å¤§å°: {model['size']} | è¯­è¨€: {model['lang']}")
        print(f"    è¯´æ˜: {model['description']}")
        print()


def interactive_download():
    """äº¤äº’å¼ä¸‹è½½æ¨¡å¼"""
    print_banner()
    print_format_choice()
    
    # é€‰æ‹©æ ¼å¼
    while True:
        format_choice = input("è¯·é€‰æ‹©æ ¼å¼ [1-2] (è¾“å…¥ 'q' é€€å‡º): ").strip()
        
        if format_choice.lower() == 'q':
            print("ğŸ‘‹ é€€å‡ºä¸‹è½½")
            return
        
        if format_choice in ['1', '2']:
            break
        
        print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥\n")
    
    model_format = 'gguf' if format_choice == '1' else 'safetensors'
    models_dict = GGUF_MODELS if format_choice == '1' else SAFETENSORS_MODELS
    
    print()
    if model_format == 'gguf':
        print_gguf_models()
    else:
        print_safetensors_models()
    
    # é€‰æ‹©æ¨¡å‹
    while True:
        model_choice = input(
            f"è¯·é€‰æ‹©è¦ä¸‹è½½çš„æ¨¡å‹ [1-{len(models_dict)}] (è¾“å…¥ 'q' é€€å‡º): "
        ).strip()
        
        if model_choice.lower() == 'q':
            print("ğŸ‘‹ é€€å‡ºä¸‹è½½")
            return
        
        if model_choice in models_dict:
            break
        
        print("âŒ æ— æ•ˆçš„é€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥\n")
    
    model = models_dict[model_choice]
    
    print(f"\nâœ… ä½ é€‰æ‹©äº†: {model['name']}")
    print(f"ğŸ“¦ æ¨¡å‹ID: {model['id']}")
    print(f"ğŸ’¾ å¤§å°: {model['size']}")
    
    if model_format == 'gguf':
        print(f"ğŸ”§ é‡åŒ–: {model['quant']}")
    
    # ç¡®è®¤ä¸‹è½½
    confirm = input(f"\nç¡®è®¤ä¸‹è½½? [Y/n]: ").strip().lower()
    if confirm not in ['', 'y', 'yes']:
        print("âŒ å–æ¶ˆä¸‹è½½")
        return
    
    try:
        print(f"\nâ³ å¼€å§‹ä¸‹è½½ {model['name']}...")
        print("ğŸ’¡ æç¤º: å¤§æ–‡ä»¶ä¸‹è½½å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")
        print()
        
        if model_format == 'gguf':
            downloaded_path = download_llm_model(
                model_id=model['id'],
                cache_dir='./models',
                model_format='gguf',
                quantization=model['quant']
            )
        else:
            downloaded_path = download_llm_model(
                model_id=model['id'],
                cache_dir='./models',
                model_format='safetensors'
            )
        
        print()
        print("=" * 70)
        print(f"âœ… ä¸‹è½½æˆåŠŸï¼")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {downloaded_path}")
        print()
        print("ğŸ“‹ ä¸‹ä¸€æ­¥ï¼š")
        print("  1. è¿è¡Œé…ç½®è„šæœ¬: python scripts/setup_llm.py")
        print("  2. åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® USE_LOCAL_LLM=true")
        print("  3. å¯åŠ¨ä½ çš„åº”ç”¨ç¨‹åº")
        print("=" * 70)
        
    except Exception as e:
        print()
        print("=" * 70)
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        print()
        print("ğŸ’¡ æ•…éšœæ’é™¤:")
        print("  1. æ£€æŸ¥ç½‘ç»œè¿æ¥")
        print("  2. æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦å……è¶³")
        print("  3. å¦‚æœæ˜¯è®¤è¯é”™è¯¯ï¼Œè®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡")
        print("  4. å°è¯•ä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼æŒ‡å®šè‡ªå®šä¹‰æ¨¡å‹ID")
        print("=" * 70)


def download_model_with_shortcut(model_shortcut='qwen2.5-7b', model_format='gguf', quantization='Q4_K_M', verbose=True):
    """ä½¿ç”¨ç®€ç§°ä¸‹è½½æ¨¡å‹ï¼ˆä¾›å…¶ä»–è„šæœ¬è°ƒç”¨ï¼‰
    
    Args:
        model_shortcut: æ¨¡å‹ç®€ç§° (qwen2.5-7b, mistral-7bç­‰)
        model_format: æ¨¡å‹æ ¼å¼ (ggufæˆ–safetensors)
        quantization: GGUFé‡åŒ–çº§åˆ« (Q4_K_M, Q5_K_Mç­‰)
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        str: ä¸‹è½½çš„æ¨¡å‹è·¯å¾„
    
    Raises:
        ValueError: ä¸æ”¯æŒçš„æ¨¡å‹ç®€ç§°
        Exception: ä¸‹è½½å¤±è´¥
    """
    # æ ¹æ®æ ¼å¼é€‰æ‹©æ­£ç¡®çš„æ¨¡å‹åˆ—è¡¨
    if model_format == 'gguf':
        model_list = GGUF_MODELS
    else:
        model_list = SAFETENSORS_MODELS
    
    # æŸ¥æ‰¾åŒ¹é…çš„æ¨¡å‹
    model_info = None
    for key, info in model_list.items():
        if info.get('short') == model_shortcut:
            model_info = info
            break
    
    if not model_info:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç®€ç§°: {model_shortcut}. å¯ç”¨: {[m['short'] for m in model_list.values() if 'short' in m]}")
    
    model_id = model_info['id']
    
    if verbose:
        print(f"ğŸ” æ¨¡å‹: {model_shortcut} ({model_info['name']})")
        print(f"ğŸ”— ä»“åº“: {model_id}")
        print(f"ğŸ“ æ ¼å¼: {model_format}")
        if model_format == 'gguf':
            print(f"ğŸ”§ é‡åŒ–: {quantization}")
    
    # è°ƒç”¨ä¸‹è½½
    from utils.model_manager.downloader import download_llm_model
    
    downloaded_path = download_llm_model(
        model_id=model_id,
        cache_dir='./models',
        model_format=model_format,
        quantization=quantization if model_format == 'gguf' else None
    )
    
    return downloaded_path


def command_line_download(args):
    """å‘½ä»¤è¡Œä¸‹è½½æ¨¡å¼"""
    print_banner()
    
    model_id = args.model_id
    model_format = args.format
    quantization = args.quant
    
    # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨ç®€ç§°
    if model_id in MODEL_SHORTCUTS:
        print(f"ğŸ” è¯†åˆ«åˆ°ç®€ç§°: {model_id}")
        model_info = MODEL_SHORTCUTS[model_id]
        model_id = model_info['id']
        print(f"ğŸ“¦ å¯¹åº”æ¨¡å‹: {model_info['name']}")
        print(f"ğŸ”— æ¨¡å‹ID: {model_id}")
    
    print(f"ğŸ“ æ ¼å¼: {model_format}")
    if model_format == 'gguf':
        print(f"ğŸ”§ é‡åŒ–: {quantization}")
    
    try:
        print(f"\nâ³ å¼€å§‹ä¸‹è½½...")
        
        from utils.model_manager.downloader import download_llm_model
        downloaded_path = download_llm_model(
            model_id=model_id,
            cache_dir='./models',
            model_format=model_format,
            quantization=quantization
        )
        
        print()
        print("=" * 70)
        print(f"âœ… ä¸‹è½½æˆåŠŸï¼")
        print(f"ğŸ“ æ¨¡å‹è·¯å¾„: {downloaded_path}")
        print("=" * 70)
        
    except Exception as e:
        print()
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        sys.exit(1)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description='TinyMem0 LLMæ¨¡å‹ä¸‹è½½å·¥å…·',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

  # äº¤äº’å¼æ¨¡å¼ï¼ˆæ¨èæ–°æ‰‹ï¼‰
  python scripts/download_llm.py

  # ä½¿ç”¨ç®€ç§°ä¸‹è½½GGUFæ¨¡å‹
  python scripts/download_llm.py --model qwen2.5-7b --format gguf
  python scripts/download_llm.py --model mistral-7b --format gguf

  # ä½¿ç”¨ç®€ç§°ä¸‹è½½SafeTensorsæ¨¡å‹  
  python scripts/download_llm.py --model qwen2.5-7b --format safetensors

  # ä½¿ç”¨å®Œæ•´IDä¸‹è½½
  python scripts/download_llm.py \\
    --model Qwen/Qwen2.5-7B-Instruct-GGUF \\
    --format gguf \\
    --quant Q4_K_M

å¯ç”¨ç®€ç§°: qwen2.5-7b, mistral-7b, llama3-8b, yi-6b
        """
    )
    
    parser.add_argument(
        '--model', '-m',
        type=str,
        dest='model_id',
        help='æ¨¡å‹ç®€ç§°æˆ–HuggingFaceæ¨¡å‹ID (å¦‚: qwen2.5-7b, mistral-7b, æˆ–å®Œæ•´ID)'
    )
    
    parser.add_argument(
        '--format',
        type=str,
        choices=['gguf', 'safetensors', 'auto'],
        default='auto',
        help='æ¨¡å‹æ ¼å¼ (é»˜è®¤: autoè‡ªåŠ¨æ£€æµ‹)'
    )
    
    parser.add_argument(
        '--quant',
        type=str,
        default='Q4_K_M',
        help='GGUFé‡åŒ–ç‰ˆæœ¬ (é»˜è®¤: Q4_K_M)'
    )
    
    args = parser.parse_args()
    
    # å¦‚æœæŒ‡å®šäº†model-idï¼Œä½¿ç”¨å‘½ä»¤è¡Œæ¨¡å¼
    if args.model_id:
        command_line_download(args)
    else:
        # å¦åˆ™ä½¿ç”¨äº¤äº’å¼æ¨¡å¼
        interactive_download()


if __name__ == "__main__":
    main()
