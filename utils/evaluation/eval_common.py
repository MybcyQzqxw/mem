#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è¯„æµ‹å…±äº«æ¨¡å— (Evaluation Common)
æä¾› memory_ingestion.py å’Œ memory_qa.py å…±ç”¨çš„é…ç½®ã€å·¥å…·å‡½æ•°å’Œå¸¸é‡

ç¡®ä¿ä¸¤ä¸ªè„šæœ¬ä½¿ç”¨å®Œå…¨ä¸€è‡´çš„ï¼š
- é…ç½®åŠ è½½é€»è¾‘
- è®°å¿†åº“è·¯å¾„/å‘½åè§„åˆ™
- æ—¥å¿—æ ¼å¼
"""

import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional
from dotenv import load_dotenv

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))
sys.path.insert(0, str(PROJECT_ROOT / 'src'))
sys.path.insert(0, str(PROJECT_ROOT / 'utils'))


# =============================================================================
# å¸¸é‡å®šä¹‰ï¼ˆç¡®ä¿ä¸¤ä¸ªè„šæœ¬ä½¿ç”¨ä¸€è‡´çš„å‘½åè§„åˆ™ï¼‰
# =============================================================================

# Qdrant è·¯å¾„æ¨¡æ¿ï¼š./qdrant_data_{speaker_name}
QDRANT_PATH_TEMPLATE = "./qdrant_data_{speaker}"

# Collection åç§°æ¨¡æ¿ï¼šlocomo_{speaker}_{conv_idx}
COLLECTION_NAME_TEMPLATE = "locomo_{speaker}_{conv_idx}"

# é»˜è®¤æ•°æ®é›†è·¯å¾„
DEFAULT_DATA_PATH = "locomo/data/locomo10.json"
FALLBACK_DATA_PATH = "locomo/data/locomo1.json"


# =============================================================================
# å·¥å…·å‡½æ•°
# =============================================================================

def str_to_bool(value: str) -> bool:
    """å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºå¸ƒå°”å€¼"""
    return value.lower() in ('true', '1', 'yes', 'on')


def print_separator(char: str = "=", length: int = 80):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)


def print_header(title: str, level: int = 1):
    """
    æ‰“å°æ ‡é¢˜
    
    Args:
        title: æ ‡é¢˜æ–‡æœ¬
        level: æ ‡é¢˜çº§åˆ« (1=ä¸»æ ‡é¢˜, 2=æ¬¡æ ‡é¢˜, 3=å°æ ‡é¢˜)
    """
    if level == 1:
        print_separator("=")
        print(f"  {title}")
        print_separator("=")
    elif level == 2:
        print_separator("-")
        print(f"  {title}")
        print_separator("-")
    else:
        print(f"\nâ–¶ {title}")


def format_timestamp() -> str:
    """æ ¼å¼åŒ–å½“å‰æ—¶é—´æˆ³ï¼ˆæ¯«ç§’ç²¾åº¦ï¼‰"""
    return datetime.now().strftime("%H:%M:%S.%f")[:-3]


def get_qdrant_path(speaker: str) -> str:
    """
    è·å–æŒ‡å®š speaker çš„ Qdrant å­˜å‚¨è·¯å¾„
    
    Args:
        speaker: speaker åç§°ï¼ˆå¦‚ Carolineï¼‰
        
    Returns:
        Qdrant è·¯å¾„ï¼ˆå¦‚ ./qdrant_data_Carolineï¼‰
    """
    return QDRANT_PATH_TEMPLATE.format(speaker=speaker)


def get_collection_name(speaker: str, conv_idx: int) -> str:
    """
    è·å–æŒ‡å®š speaker å’Œ conversation çš„ collection åç§°
    
    Args:
        speaker: speaker åç§°ï¼ˆå¦‚ Carolineï¼‰
        conv_idx: conversation ç´¢å¼•ï¼ˆ0-9ï¼‰
        
    Returns:
        Collection åç§°ï¼ˆå¦‚ locomo_Caroline_0ï¼‰
    """
    return COLLECTION_NAME_TEMPLATE.format(speaker=speaker, conv_idx=conv_idx)


def get_user_id(speaker: str, conv_idx: int) -> str:
    """
    è·å–æŒ‡å®š speaker çš„ user_id
    
    Args:
        speaker: speaker åç§°
        conv_idx: conversation ç´¢å¼•
        
    Returns:
        user_idï¼ˆå¦‚ Caroline_0ï¼‰
    """
    return f"{speaker}_{conv_idx}"


# =============================================================================
# é…ç½®åŠ è½½
# =============================================================================

def load_config() -> Dict[str, Any]:
    """
    ä» .env åŠ è½½è¯„æµ‹é…ç½®
    
    Returns:
        é…ç½®å­—å…¸ï¼ŒåŒ…å«ï¼š
        - use_local_llm: æ˜¯å¦ä½¿ç”¨æœ¬åœ° LLM
        - local_model_path: æœ¬åœ°æ¨¡å‹è·¯å¾„
        - local_embedding_model: åµŒå…¥æ¨¡å‹åç§°
        - embedding_dim: åµŒå…¥ç»´åº¦
        - batch_size: æ‰¹æ¬¡å¤§å°
        - test_mode: æ˜¯å¦æµ‹è¯•æ¨¡å¼
        - data_path: æ•°æ®é›†è·¯å¾„
    """
    load_dotenv()
    
    config = {
        "use_local_llm": str_to_bool(os.getenv('USE_LOCAL_LLM', 'false')),
        "local_model_path": None,
        "local_embedding_model": os.getenv('LOCAL_EMBEDDING_MODEL', 'BAAI/bge-small-zh-v1.5'),
        "embedding_dim": int(os.getenv('EMBEDDING_DIM', '512')),
        "memory_search_limit": int(os.getenv('MEMORY_SEARCH_LIMIT', '5')),
        "qa_search_limit": int(os.getenv('QA_SEARCH_LIMIT', '5')),
        "batch_size": int(os.getenv('EVAL_BATCH_SIZE', '2')),
        "test_mode": str_to_bool(os.getenv('EVAL_TEST_MODE', 'true')),
        "data_path": DEFAULT_DATA_PATH
    }
    
    # å¦‚æœä½¿ç”¨æœ¬åœ° LLMï¼Œæ„å»ºæ¨¡å‹è·¯å¾„
    if config["use_local_llm"]:
        model_shortcut = os.getenv('MODEL_SHORTCUT', 'mistral-7b')
        model_format = os.getenv('MODEL_FORMAT', 'gguf')
        quantization = os.getenv('MODEL_QUANTIZATION', 'Q4_K_M')
        
        try:
            from model_manager import _load_model_shortcuts
            shortcuts = _load_model_shortcuts()
            if model_shortcut in shortcuts:
                model_info = shortcuts[model_shortcut]
                model_id = model_info.get(model_format, model_shortcut)
                
                if model_format == 'gguf':
                    model_name = model_id.split('/')[-1].replace('-GGUF', '').lower()
                    filename = f"{model_name}.{quantization}.gguf"
                    config["local_model_path"] = f"models/gguf/{filename}"
                else:
                    config["local_model_path"] = f"models/safetensors/{model_id}"
        except ImportError:
            pass
    
    return config


def print_config(config: Dict[str, Any], extra_info: Optional[Dict[str, Any]] = None):
    """
    æ‰“å°é…ç½®ä¿¡æ¯
    
    Args:
        config: é…ç½®å­—å…¸
        extra_info: é¢å¤–ä¿¡æ¯ï¼ˆå¦‚ max_conversationsï¼‰
    """
    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   â€¢ æ•°æ®é›†: {config['data_path']}")
    print(f"   â€¢ batch_size: {config.get('batch_size', 'N/A')}")
    print(f"   â€¢ memory_search_limit: {config.get('memory_search_limit', 5)}")
    print(f"   â€¢ qa_search_limit: {config.get('qa_search_limit', 5)}")
    print(f"   â€¢ æœ¬åœ°LLM: {config['use_local_llm']}")
    if config['use_local_llm'] and config['local_model_path']:
        print(f"   â€¢ æ¨¡å‹è·¯å¾„: {config['local_model_path']}")
    print(f"   â€¢ åµŒå…¥æ¨¡å‹: {config['local_embedding_model']}")
    
    if extra_info:
        for key, value in extra_info.items():
            print(f"   â€¢ {key}: {value}")


# =============================================================================
# åŸºç¡€æ—¥å¿—è®°å½•å™¨
# =============================================================================

class BaseLogger:
    """åŸºç¡€æ—¥å¿—è®°å½•å™¨"""
    
    # æ—¥å¿—çº§åˆ«å‰ç¼€æ˜ å°„
    LEVEL_PREFIXES = {
        "INFO": "â„¹ï¸ ",
        "SUCCESS": "âœ…",
        "WARN": "âš ï¸ ",
        "ERROR": "âŒ",
        "BATCH": "ğŸ“¦",
        "MEMORY": "ğŸ§ ",
        "SEARCH": "ğŸ”",
        "QA": "â“",
        "ANSWER": "ğŸ’¬"
    }
    
    def __init__(self, verbose: bool = True):
        self.verbose = verbose
        self.stats = {}
    
    def log(self, level: str, message: str, **kwargs):
        """
        è®°å½•æ—¥å¿—
        
        Args:
            level: æ—¥å¿—çº§åˆ«ï¼ˆINFO, SUCCESS, WARN, ERROR, BATCH, MEMORY, SEARCH, QA, ANSWERï¼‰
            message: æ—¥å¿—æ¶ˆæ¯
            **kwargs: é¢å¤–ä¿¡æ¯ï¼ˆä¼šåœ¨æ¶ˆæ¯ä¸‹æ–¹æ˜¾ç¤ºï¼‰
        """
        timestamp = format_timestamp()
        prefix = self.LEVEL_PREFIXES.get(level, "  ")
        
        print(f"[{timestamp}] {prefix} {message}")
        
        if kwargs and self.verbose:
            for key, value in kwargs.items():
                if isinstance(value, str) and len(value) > 100:
                    value = value[:100] + "..."
                print(f"           â””â”€ {key}: {value}")
    
    def print_stats(self, title: str = "ç»Ÿè®¡"):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print_header(title, level=2)
        for key, value in self.stats.items():
            # å°† snake_case è½¬æ¢ä¸ºæ›´å‹å¥½çš„æ˜¾ç¤º
            display_key = key.replace("_", " ").title()
            print(f"  ğŸ“Š {display_key}: {value}")


# =============================================================================
# è®°å¿†ç³»ç»Ÿå·¥å‚å‡½æ•°
# =============================================================================

def create_memory_system(
    speaker: str,
    conv_idx: int,
    config: Dict[str, Any]
):
    """
    åˆ›å»ºè®°å¿†ç³»ç»Ÿå®ä¾‹
    
    ç¡®ä¿ memory_ingestion.py å’Œ memory_qa.py ä½¿ç”¨å®Œå…¨ä¸€è‡´çš„å‚æ•°åˆ›å»ºè®°å¿†åº“
    
    Args:
        speaker: speaker åç§°
        conv_idx: conversation ç´¢å¼•
        config: é…ç½®å­—å…¸
        
    Returns:
        MemorySystem å®ä¾‹
    """
    from tinymem0.memory_system import MemorySystem
    
    return MemorySystem(
        collection_name=get_collection_name(speaker, conv_idx),
        qdrant_path=get_qdrant_path(speaker),
        use_local_llm=config['use_local_llm'],
        local_model_path=config['local_model_path'],
        local_embedding_model=config['local_embedding_model'],
        embedding_dim=config['embedding_dim'],
        memory_search_limit=config.get('memory_search_limit', 5)
    )
