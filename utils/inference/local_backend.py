#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æœ¬åœ°LLMæ¨ç†å¼•æ“æ¨¡å—
æ”¯æŒå¤šç§æ¨¡å‹æ ¼å¼ï¼š
- GGUF: é‡åŒ–æ¨¡å‹ï¼Œä½¿ç”¨ llama-cpp-python
- SafeTensors: åŸå§‹ç²¾åº¦æ¨¡å‹ï¼Œä½¿ç”¨ transformers
"""

import os
from pathlib import Path
from typing import Optional, List, Dict, Literal


class LocalLLM:
    """ç»Ÿä¸€çš„æœ¬åœ°LLMæ¨ç†å¼•æ“ï¼ˆæ”¯æŒGGUFå’ŒSafeTensorsæ ¼å¼ï¼‰"""
    
    def __init__(
        self, 
        model_path: Optional[str] = None, 
        backend: Literal['auto', 'gguf', 'transformers'] = 'auto',
        n_ctx: int = 4096, 
        n_gpu_layers: int = -1, 
        device: str = 'auto',
        verbose: bool = False
    ):
        """
        åˆå§‹åŒ–æœ¬åœ°LLMæ¨ç†å¼•æ“
        
        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼ˆæ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„ï¼‰ï¼Œå¦‚æœä¸ºNoneåˆ™ä»ç¯å¢ƒå˜é‡LOCAL_MODEL_PATHè¯»å–
            backend: æ¨ç†åç«¯ ('auto', 'gguf', 'transformers')
                - 'auto': è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ ¼å¼
                - 'gguf': ä½¿ç”¨ llama-cpp-python (ç”¨äº.ggufæ–‡ä»¶)
                - 'transformers': ä½¿ç”¨ HuggingFace transformers (ç”¨äºsafetensorsç›®å½•)
            n_ctx: ä¸Šä¸‹æ–‡çª—å£å¤§å° (ä»…ç”¨äºgguf)
            n_gpu_layers: GPUåŠ é€Ÿå±‚æ•°ï¼Œ-1è¡¨ç¤ºå…¨éƒ¨ (ä»…ç”¨äºgguf)
            device: è®¾å¤‡ ('auto', 'cuda', 'cpu') (ä»…ç”¨äºtransformers)
            verbose: æ˜¯å¦è¾“å‡ºè¯¦ç»†æ—¥å¿—
        """
        self.model_path = model_path or os.getenv("LOCAL_MODEL_PATH")
        
        if not self.model_path:
            raise ValueError("è¯·è®¾ç½®model_pathå‚æ•°æˆ–LOCAL_MODEL_PATHç¯å¢ƒå˜é‡")
        
        # æ ‡å‡†åŒ–è·¯å¾„
        self.model_path = self._resolve_model_path(self.model_path)
        
        # è‡ªåŠ¨æ£€æµ‹åç«¯
        self.backend = self._detect_backend(backend)
        
        print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
        print(f"ğŸ”§ ä½¿ç”¨åç«¯: {self.backend}")
        
        # æ ¹æ®åç«¯åŠ è½½æ¨¡å‹
        if self.backend == 'gguf':
            self._load_gguf_model(n_ctx, n_gpu_layers, verbose)
        elif self.backend == 'transformers':
            self._load_transformers_model(device, verbose)
        
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def _resolve_model_path(self, path: str) -> str:
        """è§£ææ¨¡å‹è·¯å¾„ï¼Œæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„"""
        if os.path.exists(path):
            return path
        
        # å°è¯•åœ¨æ ‡å‡†ç›®å½•ä¸‹æŸ¥æ‰¾
        search_dirs = [
            'models/gguf',
            'models/safetensors', 
            'models',
            '.'
        ]
        
        for base_dir in search_dirs:
            potential_path = os.path.join(base_dir, path)
            if os.path.exists(potential_path):
                return potential_path
        
        raise FileNotFoundError(
            f"æ‰¾ä¸åˆ°æ¨¡å‹: {path}\n"
            f"å·²æœç´¢ç›®å½•: {', '.join(search_dirs)}"
        )
    
    def _detect_backend(self, backend: str) -> str:
        """è‡ªåŠ¨æ£€æµ‹æ¨ç†åç«¯"""
        if backend != 'auto':
            return backend
        
        path = Path(self.model_path)
        
        # æ£€æµ‹GGUFæ–‡ä»¶
        if path.is_file() and path.suffix.lower() == '.gguf':
            return 'gguf'
        
        # æ£€æµ‹SafeTensorsç›®å½•
        if path.is_dir():
            has_safetensors = any(
                f.name.endswith('.safetensors') 
                for f in path.iterdir() 
                if f.is_file()
            )
            has_config = (path / 'config.json').exists()
            
            if has_safetensors and has_config:
                return 'transformers'
        
        raise ValueError(
            f"æ— æ³•è‡ªåŠ¨æ£€æµ‹æ¨¡å‹æ ¼å¼: {self.model_path}\n"
            f"è¯·æ˜ç¡®æŒ‡å®š backend='gguf' æˆ– backend='transformers'"
        )
    
    def _load_gguf_model(self, n_ctx: int, n_gpu_layers: int, verbose: bool):
        """åŠ è½½GGUFæ ¼å¼æ¨¡å‹"""
        try:
            from llama_cpp import Llama
        except ImportError:
            raise ImportError(
                "GGUFåç«¯éœ€è¦ llama-cpp-python\n"
                "å®‰è£…: pip install llama-cpp-python>=0.2.0"
            )
        
        self.model = Llama(
            model_path=self.model_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            verbose=verbose
        )
        self.tokenizer = None
    
    def _load_transformers_model(self, device: str, verbose: bool):
        """åŠ è½½SafeTensorsæ ¼å¼æ¨¡å‹"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch
        except ImportError:
            raise ImportError(
                "Transformersåç«¯éœ€è¦ transformers å’Œ torch\n"
                "å®‰è£…: pip install transformers>=4.35.0 torch>=2.0.0"
            )
        
        # è®¾å¤‡é€‰æ‹©
        if device == 'auto':
            device = 'cuda' if torch.cuda.is_available() else 'cpu'
        
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.model_path,
            trust_remote_code=True
        )
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.model_path,
            torch_dtype=torch.float16 if device == 'cuda' else torch.float32,
            device_map=device,
            trust_remote_code=True
        )
        
        self.device = device
        print(f"ğŸ¯ ä½¿ç”¨è®¾å¤‡: {device}")
    
    def generate(self,
                 system_prompt: str,
                 user_content: str,
                 max_tokens: int = 512,
                 temperature: float = 0.7,
                 top_p: float = 0.9,
                 stop: Optional[List[str]] = None) -> Optional[str]:
        """
        ç”Ÿæˆå›å¤ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            user_content: ç”¨æˆ·è¾“å…¥å†…å®¹
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus samplingå‚æ•°
            stop: åœæ­¢è¯åˆ—è¡¨
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if self.backend == 'gguf':
            return self._generate_gguf(system_prompt, user_content, max_tokens, temperature, top_p, stop)
        elif self.backend == 'transformers':
            return self._generate_transformers(system_prompt, user_content, max_tokens, temperature, top_p)
    
    def _generate_gguf(self,
                       system_prompt: str,
                       user_content: str,
                       max_tokens: int,
                       temperature: float,
                       top_p: float,
                       stop: Optional[List[str]]) -> Optional[str]:
        """GGUFåç«¯çš„ç”Ÿæˆå®ç°"""
        try:
            # JSONçº¦æŸå¢å¼º
            enhanced_system = system_prompt
            if 'json' in system_prompt.lower() or 'JSON' in system_prompt:
                enhanced_system = system_prompt + "\n\nIMPORTANT: You MUST respond with valid JSON only. Do not include any explanatory text before or after the JSON object."
            
            # æ„å»ºQwenæ ¼å¼çš„prompt
            prompt = f"""<|im_start|>system
{enhanced_system}<|im_end|>
<|im_start|>user
{user_content}<|im_end|>
<|im_start|>assistant
"""
            
            response = self.model(
                prompt,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                stop=stop or ["<|im_end|>"],
                echo=False
            )
            
            if response and 'choices' in response and len(response['choices']) > 0:
                return response['choices'][0]['text'].strip()
            return None
            
        except Exception as e:
            print(f"âŒ GGUFç”Ÿæˆå¼‚å¸¸: {e}")
            return None
    
    def _generate_transformers(self,
                               system_prompt: str,
                               user_content: str,
                               max_tokens: int,
                               temperature: float,
                               top_p: float) -> Optional[str]:
        """Transformersåç«¯çš„ç”Ÿæˆå®ç°"""
        try:
            # æ„å»ºå¯¹è¯æ ¼å¼
            messages = [
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': user_content}
            ]
            
            # ä½¿ç”¨tokenizerçš„èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer([text], return_tensors="pt").to(self.device)
            
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True
            )
            
            # è§£ç è¾“å‡º
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return generated_text.strip()
            
        except Exception as e:
            print(f"âŒ Transformersç”Ÿæˆå¼‚å¸¸: {e}")
            return None
    
    def chat(self, messages: List[Dict[str, str]],
             max_tokens: int = 512,
             temperature: float = 0.7,
             top_p: float = 0.9) -> Optional[str]:
        """
        èŠå¤©æ¥å£ï¼ˆç»Ÿä¸€æ¥å£ï¼‰
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å«roleå’Œcontent
            max_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
            temperature: æ¸©åº¦å‚æ•°
            top_p: nucleus samplingå‚æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if self.backend == 'gguf':
            return self._chat_gguf(messages, max_tokens, temperature, top_p)
        elif self.backend == 'transformers':
            return self._chat_transformers(messages, max_tokens, temperature, top_p)
    
    def _chat_gguf(self,
                   messages: List[Dict[str, str]],
                   max_tokens: int,
                   temperature: float,
                   top_p: float) -> Optional[str]:
        """GGUFåç«¯çš„èŠå¤©å®ç°"""
        try:
            # JSONçº¦æŸå¢å¼º
            enhanced_messages = messages.copy()
            if enhanced_messages and enhanced_messages[0]['role'] == 'system':
                system_content = enhanced_messages[0]['content']
                if 'json' in system_content.lower():
                    enhanced_messages[0]['content'] = (
                        system_content +
                        "\n\nCRITICAL: You MUST respond ONLY with valid JSON. "
                        "Start with { and end with }. Do NOT include any text "
                        "before or after the JSON object."
                    )
            
            response = self.model.create_chat_completion(
                messages=enhanced_messages,
                max_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p
            )
            
            if response and 'choices' in response and len(response['choices']) > 0:
                return response['choices'][0]['message']['content'].strip()
            return None
            
        except Exception as e:
            print(f"âŒ GGUFèŠå¤©å¼‚å¸¸: {e}")
            return None
    
    def _chat_transformers(self,
                           messages: List[Dict[str, str]],
                           max_tokens: int,
                           temperature: float,
                           top_p: float) -> Optional[str]:
        """Transformersåç«¯çš„èŠå¤©å®ç°"""
        try:
            # ä½¿ç”¨tokenizerçš„èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            inputs = self.tokenizer([text], return_tensors="pt")
            inputs = {k: v.to(self.device) for k, v in inputs.items()}
            
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                top_p=top_p,
                do_sample=True
            )
            
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return generated_text.strip()
            
        except Exception as e:
            print(f"âŒ TransformersèŠå¤©å¼‚å¸¸: {e}")
            return None


# å…¨å±€LLMå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
_llm_instance: Optional[LocalLLM] = None


def get_local_llm(model_path: Optional[str] = None,
                  backend: str = 'auto',
                  n_ctx: int = 4096,
                  n_gpu_layers: int = -1,
                  device: str = 'auto') -> LocalLLM:
    """
    è·å–å…¨å±€LLMå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
    é¿å…é‡å¤åŠ è½½æ¨¡å‹ï¼ŒèŠ‚çœå†…å­˜
    
    Args:
        model_path: æ¨¡å‹è·¯å¾„
        backend: æ¨ç†åç«¯
        n_ctx: ä¸Šä¸‹æ–‡çª—å£
        n_gpu_layers: GPUå±‚æ•°
        device: è®¾å¤‡é€‰æ‹©
        
    Returns:
        LocalLLMå®ä¾‹
    """
    global _llm_instance
    if _llm_instance is None:
        _llm_instance = LocalLLM(
            model_path=model_path,
            backend=backend,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            device=device
        )
    return _llm_instance

