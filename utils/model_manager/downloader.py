#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ¨¡å‹ä¸‹è½½å·¥å…·
æä¾›ä»å„ç§æºä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨åŠŸèƒ½
"""

import os
import sys
from typing import Optional


def download_embedding_model(model_id: str = 'AI-ModelScope/bge-small-zh-v1.5', 
                             cache_dir: str = './embedding_models',
                             source: str = 'modelscope') -> str:
    """
    ä¸‹è½½åµŒå…¥æ¨¡å‹ï¼ˆé€šç”¨å‡½æ•°ï¼‰
    
    æ”¯æŒä»å¤šä¸ªæºä¸‹è½½æ¨¡å‹ï¼š
    - ModelScope: ä¸­å›½åŒºå‹å¥½ï¼Œé€‚åˆä¸‹è½½ä¸­æ–‡æ¨¡å‹
    - HuggingFace: å›½é™…ä¸»æµæ¨¡å‹åº“
    
    Args:
        model_id: æ¨¡å‹IDï¼ˆæ ¼å¼ä¾èµ–äºsourceï¼‰
        cache_dir: æœ¬åœ°ç¼“å­˜ç›®å½•
        source: ä¸‹è½½æº ('modelscope' æˆ– 'huggingface')
        
    Returns:
        ä¸‹è½½åçš„æ¨¡å‹æœ¬åœ°è·¯å¾„
        
    Raises:
        ImportError: ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…
        RuntimeError: ä¸‹è½½å¤±è´¥
        
    Examples:
        >>> # ä»ModelScopeä¸‹è½½
        >>> path = download_embedding_model('AI-ModelScope/bge-small-zh-v1.5')
        
        >>> # ä»HuggingFaceä¸‹è½½
        >>> path = download_embedding_model(
        ...     'sentence-transformers/all-MiniLM-L6-v2',
        ...     source='huggingface'
        ... )
    """
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs(cache_dir, exist_ok=True)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    model_path = os.path.join(cache_dir, model_id)
    if os.path.exists(model_path):
        print(f"âœ… æ¨¡å‹å·²å­˜åœ¨: {model_path}")
        return model_path
    
    print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_id}")
    print(f"ğŸ“ ä¸‹è½½ç›®å½•: {cache_dir}")
    print(f"ğŸŒ ä¸‹è½½æº: {source}")
    
    try:
        if source == 'modelscope':
            return _download_from_modelscope(model_id, cache_dir)
        elif source == 'huggingface':
            return _download_from_huggingface(model_id, cache_dir)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¸‹è½½æº: {source}")
            
    except ImportError as e:
        print(f"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…")
        print(f"è¯¦ç»†ä¿¡æ¯: {e}")
        if source == 'modelscope':
            print("è¯·è¿è¡Œ: pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple")
        elif source == 'huggingface':
            print("è¯·è¿è¡Œ: pip install huggingface_hub")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        raise RuntimeError(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")


def _download_from_modelscope(model_id: str, cache_dir: str) -> str:
    """ä»ModelScopeä¸‹è½½æ¨¡å‹ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    from modelscope import snapshot_download
    
    downloaded_path = snapshot_download(
        model_id=model_id,
        cache_dir=cache_dir,
        revision='master'
    )
    
    print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {downloaded_path}")
    return downloaded_path


def _download_from_huggingface(model_id: str, cache_dir: str) -> str:
    """ä»HuggingFaceä¸‹è½½æ¨¡å‹ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    from huggingface_hub import snapshot_download
    
    downloaded_path = snapshot_download(
        repo_id=model_id,
        cache_dir=cache_dir,
        local_dir=os.path.join(cache_dir, model_id)
    )
    
    print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {downloaded_path}")
    return downloaded_path


def download_llm_model(
    model_id: str,
    cache_dir: str = './models',
    model_format: str = 'gguf'
) -> str:
    """
    LLMæ¨¡å‹ä¸‹è½½ï¼ˆå½“å‰ä¸ºå ä½å‡½æ•°ï¼Œéœ€æ‰‹åŠ¨ä¸‹è½½ï¼‰
    
    ç”±äºLLMæ¨¡å‹ä½“ç§¯å·¨å¤§(3-20GB+)ä¸”æœ‰å¤šç§é‡åŒ–ç‰ˆæœ¬ï¼Œ
    å½“å‰ç‰ˆæœ¬éœ€è¦ç”¨æˆ·æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹ã€‚
    
    Args:
        model_id: æ¨¡å‹ID
        cache_dir: ç¼“å­˜ç›®å½•
        model_format: æ¨¡å‹æ ¼å¼ ('gguf' æˆ– 'safetensors')
        
    Returns:
        ä¸‹è½½åçš„æ¨¡å‹è·¯å¾„
        
    Raises:
        NotImplementedError: å½“å‰ç‰ˆæœ¬æš‚ä¸æ”¯æŒè‡ªåŠ¨ä¸‹è½½
        
    æ¨èä¸‹è½½æºå’Œæ­¥éª¤:
    
    ==== GGUF æ ¼å¼æ¨¡å‹ (é‡åŒ–ï¼Œå†…å­˜å ç”¨ä½) ====
    
    1. HuggingFace GGUFæ¨¡å‹åº“:
       https://huggingface.co/models?library=gguf
       
       æ¨èæ¨¡å‹:
       - Qwen2-7B-Instruct-GGUF (ä¸­æ–‡ä¼˜åŒ–)
       - Mistral-7B-Instruct-GGUF (é€šç”¨æ€§å¥½)
       - Llama-3-8B-Instruct-GGUF (Metaå®˜æ–¹)
       
       é‡åŒ–ç‰ˆæœ¬é€‰æ‹©:
       - Q4_K_M: 4GBå·¦å³ï¼Œæ¨èå¹³è¡¡
       - Q5_K_M: 5GBå·¦å³ï¼Œæ›´é«˜ç²¾åº¦
       - Q8_0: 8GBå·¦å³ï¼Œæ¥è¿‘åŸå§‹ç²¾åº¦
    
    2. ModelScope GGUFæ¨¡å‹:
       https://modelscope.cn/models
       æœç´¢å…³é”®è¯ "GGUF" + æ¨¡å‹å
    
    3. ä¸‹è½½æ­¥éª¤:
       a. è®¿é—®æ¨¡å‹é¡µé¢
       b. ä¸‹è½½ .gguf æ–‡ä»¶
       c. æ”¾ç½®åˆ°: ./models/gguf/
       d. è¿è¡Œ: python scripts/setup_llm.py
    
    ==== SafeTensors æ ¼å¼æ¨¡å‹ (åŸå§‹ç²¾åº¦ï¼Œå†…å­˜å ç”¨é«˜) ====
    
    1. HuggingFace æ¨¡å‹åº“:
       https://huggingface.co/models?library=transformers
       
       æ¨èæ¨¡å‹:
       - Qwen/Qwen2-7B-Instruct
       - mistralai/Mistral-7B-Instruct-v0.2
       - meta-llama/Meta-Llama-3-8B-Instruct
    
    2. ä¸‹è½½æ­¥éª¤ (ä½¿ç”¨ huggingface-cli):
       pip install huggingface_hub
       huggingface-cli download Qwen/Qwen2-7B-Instruct --local-dir ./models/safetensors/qwen2-7b
    
    3. æˆ–ä½¿ç”¨ git-lfs:
       git lfs install
       cd models/safetensors
       git clone https://huggingface.co/Qwen/Qwen2-7B-Instruct
    
    ==== ç¡¬ä»¶è¦æ±‚ ====
    
    GGUF (é‡åŒ–):
      - 7B Q4: æœ€ä½4GBæ˜¾å­˜/RAM
      - 7B Q5: æœ€ä½6GBæ˜¾å­˜/RAM
      - 13B Q4: æœ€ä½8GBæ˜¾å­˜/RAM
    
    SafeTensors (FP16):
      - 7B: æœ€ä½14GBæ˜¾å­˜
      - 13B: æœ€ä½26GBæ˜¾å­˜
      - éœ€è¦CUDAæ”¯æŒ
    
    """
    raise NotImplementedError(
        f"\n"
        f"{'='*70}\n"
        f"LLMæ¨¡å‹éœ€è¦æ‰‹åŠ¨ä¸‹è½½\n"
        f"{'='*70}\n"
        f"\n"
        f"æ¨¡å‹æ ¼å¼: {model_format}\n"
        f"ç›®æ ‡ç›®å½•: {cache_dir}/{model_format}/\n"
        f"\n"
        f"è¯·å‚è€ƒå‡½æ•°æ–‡æ¡£ä¸­çš„ä¸‹è½½æŒ‡å—ï¼Œæˆ–è¿è¡Œ:\n"
        f"  python -c \"from utils.model_manager import download_llm_model; help(download_llm_model)\"\n"
        f"\n"
        f"ä¸‹è½½å®Œæˆåè¿è¡Œé…ç½®è„šæœ¬:\n"
        f"  python scripts/setup_llm.py\n"
        f"{'='*70}\n"
    )



def check_model_exists(model_id: str, cache_dir: str) -> bool:
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
    
    Args:
        model_id: æ¨¡å‹ID
        cache_dir: ç¼“å­˜ç›®å½•
        
    Returns:
        True if exists, False otherwise
    """
    model_path = os.path.join(cache_dir, model_id)
    return os.path.exists(model_path)
