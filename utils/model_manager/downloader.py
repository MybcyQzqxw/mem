#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
æ¨¡å‹ä¸‹è½½å·¥å…·
æä¾›ä»å„ç§æºä¸‹è½½é¢„è®­ç»ƒæ¨¡å‹çš„é€šç”¨åŠŸèƒ½
"""

import os
import sys
from typing import Optional


def download_embedding_model(model_id: str = 'AI-ModelScope/bge-small-zh-v1.5', 
                             cache_dir: str = './embedding_models',
                             source: str = 'modelscope') -> str:
    """
    ä¸‹è½½åµŒå…¥æ¨¡å‹ï¼ˆé€šç”¨å‡½æ•°ï¼‰
    
    æ”¯æŒä»å¤šä¸ªæºä¸‹è½½æ¨¡å‹ï¼š
    - ModelScope: ä¸­å›½åŒºå‹å¥½ï¼Œé€‚åˆä¸‹è½½ä¸­æ–‡æ¨¡å‹
    - HuggingFace: å›½é™…ä¸»æµæ¨¡å‹åº“
    
    Args:
        model_id: æ¨¡å‹IDï¼ˆæ ¼å¼ä¾èµ–äºsourceï¼‰
        cache_dir: æœ¬åœ°ç¼“å­˜ç›®å½•
        source: ä¸‹è½½æº ('modelscope' æˆ– 'huggingface')
        
    Returns:
        ä¸‹è½½åçš„æ¨¡å‹æœ¬åœ°è·¯å¾„
        
    Raises:
        ImportError: ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…
        RuntimeError: ä¸‹è½½å¤±è´¥
        
    Examples:
        >>> # ä»ModelScopeä¸‹è½½
        >>> path = download_embedding_model('AI-ModelScope/bge-small-zh-v1.5')
        
        >>> # ä»HuggingFaceä¸‹è½½
        >>> path = download_embedding_model(
        ...     'sentence-transformers/all-MiniLM-L6-v2',
        ...     source='huggingface'
        ... )
    """
    # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
    os.makedirs(cache_dir, exist_ok=True)
    
    # æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²å­˜åœ¨
    model_path = os.path.join(cache_dir, model_id)
    if os.path.exists(model_path):
        print(f"âœ… æ¨¡å‹å·²å­˜åœ¨: {model_path}")
        return model_path
    
    print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_id}")
    print(f"ğŸ“ ä¸‹è½½ç›®å½•: {cache_dir}")
    print(f"ğŸŒ ä¸‹è½½æº: {source}")
    
    try:
        if source == 'modelscope':
            return _download_from_modelscope(model_id, cache_dir)
        elif source == 'huggingface':
            return _download_from_huggingface(model_id, cache_dir)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¸‹è½½æº: {source}")
            
    except ImportError as e:
        print(f"âŒ é”™è¯¯: ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…")
        print(f"è¯¦ç»†ä¿¡æ¯: {e}")
        if source == 'modelscope':
            print("è¯·è¿è¡Œ: pip install modelscope -i https://pypi.tuna.tsinghua.edu.cn/simple")
        elif source == 'huggingface':
            print("è¯·è¿è¡Œ: pip install huggingface_hub")
        sys.exit(1)
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        raise RuntimeError(f"æ¨¡å‹ä¸‹è½½å¤±è´¥: {e}")


def _download_from_modelscope(model_id: str, cache_dir: str) -> str:
    """ä»ModelScopeä¸‹è½½æ¨¡å‹ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    from modelscope import snapshot_download
    
    downloaded_path = snapshot_download(
        model_id=model_id,
        cache_dir=cache_dir,
        revision='master'
    )
    
    print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {downloaded_path}")
    return downloaded_path


def _download_from_huggingface(model_id: str, cache_dir: str) -> str:
    """ä»HuggingFaceä¸‹è½½æ¨¡å‹ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    from huggingface_hub import snapshot_download
    
    downloaded_path = snapshot_download(
        repo_id=model_id,
        cache_dir=cache_dir,
        local_dir=os.path.join(cache_dir, model_id)
    )
    
    print(f"âœ… æ¨¡å‹ä¸‹è½½å®Œæˆ: {downloaded_path}")
    return downloaded_path


def download_llm_model(
    model_id: str,
    cache_dir: str = './models',
    model_format: str = 'auto',
    quantization: Optional[str] = None,
    source: str = 'huggingface'
) -> str:
    """
    ä» HuggingFace ä¸‹è½½ LLM æ¨¡å‹ï¼ˆæ”¯æŒ GGUF å’Œ SafeTensorsï¼‰
    
    Args:
        model_id: æ¨¡å‹ID
            - GGUF: å¦‚ "TheBloke/Qwen2-7B-Instruct-GGUF"
            - SafeTensors: å¦‚ "Qwen/Qwen2-7B-Instruct"
        cache_dir: ç¼“å­˜ç›®å½•åŸºç¡€è·¯å¾„
        model_format: æ¨¡å‹æ ¼å¼ ('auto', 'gguf', 'safetensors')
            - 'auto': è‡ªåŠ¨æ£€æµ‹ï¼ˆé€šè¿‡model_idåˆ¤æ–­ï¼‰
        quantization: GGUFé‡åŒ–ç‰ˆæœ¬ï¼ˆä»…GGUFæ ¼å¼éœ€è¦ï¼‰
            - 'Q4_K_M': 4-bit, æ¨è
            - 'Q5_K_M': 5-bit, æ›´é«˜ç²¾åº¦
            - 'Q8_0': 8-bit, æ¥è¿‘åŸå§‹
        source: ä¸‹è½½æº ('huggingface' æˆ– 'modelscope')
        
    Returns:
        ä¸‹è½½åçš„æ¨¡å‹è·¯å¾„
        
    Examples:
        >>> # ä¸‹è½½GGUFæ¨¡å‹
        >>> path = download_llm_model(
        ...     "TheBloke/Qwen2-7B-Instruct-GGUF",
        ...     quantization="Q4_K_M"
        ... )
        
        >>> # ä¸‹è½½SafeTensorsæ¨¡å‹
        >>> path = download_llm_model(
        ...     "Qwen/Qwen2-7B-Instruct",
        ...     model_format="safetensors"
        ... )
    """
    # è‡ªåŠ¨æ£€æµ‹æ ¼å¼
    if model_format == 'auto':
        if 'GGUF' in model_id or 'gguf' in model_id:
            model_format = 'gguf'
        else:
            model_format = 'safetensors'
    
    print(f"ğŸ“¦ å¼€å§‹ä¸‹è½½ {model_format.upper()} æ ¼å¼æ¨¡å‹: {model_id}")
    
    if model_format == 'gguf':
        return _download_gguf_model(model_id, cache_dir, quantization, source)
    elif model_format == 'safetensors':
        return _download_safetensors_model(model_id, cache_dir, source)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹æ ¼å¼: {model_format}")


def _download_gguf_model(
    model_id: str,
    cache_dir: str,
    quantization: Optional[str],
    source: str
) -> str:
    """ä¸‹è½½GGUFæ ¼å¼æ¨¡å‹ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    try:
        from huggingface_hub import hf_hub_download
    except ImportError:
        raise ImportError(
            "éœ€è¦å®‰è£… huggingface_hub\n"
            "è¿è¡Œ: pip install huggingface_hub>=0.19.0"
        )
    
    # è®¾ç½®ç›®æ ‡ç›®å½•
    target_dir = os.path.join(cache_dir, 'gguf')
    os.makedirs(target_dir, exist_ok=True)
    
    # å¦‚æœæœªæŒ‡å®šé‡åŒ–ç‰ˆæœ¬ï¼Œå°è¯•æ¨è
    if not quantization:
        print("âš ï¸  æœªæŒ‡å®šé‡åŒ–ç‰ˆæœ¬ï¼Œæ¨èä½¿ç”¨ Q4_K_M")
        quantization = "Q4_K_M"
    
    # æ„å»ºGGUFæ–‡ä»¶åï¼ˆé€šå¸¸æ ¼å¼ï¼šæ¨¡å‹å-é‡åŒ–ç‰ˆæœ¬.ggufï¼‰
    # éœ€è¦åˆ—å‡ºä»“åº“æ–‡ä»¶æ¥æ‰¾åˆ°ç²¾ç¡®æ–‡ä»¶å
    print(f"ğŸ” æ­£åœ¨æŸ¥æ‰¾ {quantization} é‡åŒ–ç‰ˆæœ¬...")
    
    try:
        from huggingface_hub import list_repo_files
        
        print("   è¿æ¥åˆ° HuggingFace...")
        # åˆ—å‡ºä»“åº“ä¸­æ‰€æœ‰æ–‡ä»¶
        files = list_repo_files(model_id)
        gguf_files = [f for f in files if f.endswith('.gguf')]
        
        print(f"   æ‰¾åˆ° {len(gguf_files)} ä¸ªGGUFæ–‡ä»¶")
        
        # æŸ¥æ‰¾åŒ¹é…çš„é‡åŒ–æ–‡ä»¶
        target_file = None
        
        # ä¼˜å…ˆåŒ¹é…å®Œæ•´çš„é‡åŒ–åç§°ï¼ˆå¦‚ Q4_K_Mï¼‰
        for f in gguf_files:
            if quantization.upper() in f.upper():
                target_file = f
                break
        
        # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•åŒ¹é…ç±»ä¼¼çš„ï¼ˆå¦‚ Q4_K_M -> q4_k_mï¼‰
        if not target_file:
            quant_normalized = quantization.replace('_', '-').lower()
            for f in gguf_files:
                f_normalized = f.replace('_', '-').lower()
                if quant_normalized in f_normalized:
                    target_file = f
                    break
        
        if not target_file:
            print(f"\nâŒ æœªæ‰¾åˆ° {quantization} é‡åŒ–ç‰ˆæœ¬")
            print(f"\nå¯ç”¨çš„GGUFæ–‡ä»¶:")
            for f in gguf_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                print(f"  - {f}")
            raise ValueError(
                f"æœªæ‰¾åˆ° {quantization} é‡åŒ–ç‰ˆæœ¬\n"
                f"è¯·ä»ä¸Šé¢çš„åˆ—è¡¨ä¸­é€‰æ‹©ä¸€ä¸ªæ–‡ä»¶"
            )
        
        print(f"âœ… æ‰¾åˆ°æ–‡ä»¶: {target_file}")
        
        # è·å–æ–‡ä»¶å¤§å°
        from huggingface_hub import HfApi
        api = HfApi()
        file_info = api.repo_info(model_id, files_metadata=True)
        file_size = None
        for sibling in file_info.siblings:
            if sibling.rfilename == target_file:
                file_size = sibling.size
                break
        
        if file_size:
            size_gb = file_size / (1024**3)
            print(f"ğŸ“¦ æ–‡ä»¶å¤§å°: {size_gb:.2f} GB")
        
        print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½...")
        print(f"ğŸ’¾ ä¿å­˜åˆ°: {target_dir}")
        
        # ä¸‹è½½æ–‡ä»¶ï¼ˆå¸¦è¿›åº¦æ¡ï¼‰
        downloaded_path = hf_hub_download(
            repo_id=model_id,
            filename=target_file,
            local_dir=target_dir
        )
        
        print(f"\nâœ… ä¸‹è½½å®Œæˆ: {downloaded_path}")
        return downloaded_path
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        raise


def _download_safetensors_model(
    model_id: str,
    cache_dir: str,
    source: str
) -> str:
    """ä¸‹è½½SafeTensorsæ ¼å¼æ¨¡å‹ï¼ˆå†…éƒ¨å‡½æ•°ï¼‰"""
    try:
        from huggingface_hub import snapshot_download
    except ImportError:
        raise ImportError(
            "éœ€è¦å®‰è£… huggingface_hub\n"
            "è¿è¡Œ: pip install huggingface_hub>=0.19.0"
        )
    
    # è®¾ç½®ç›®æ ‡ç›®å½•
    model_name = model_id.split('/')[-1]
    target_dir = os.path.join(cache_dir, 'safetensors', model_name)
    os.makedirs(os.path.dirname(target_dir), exist_ok=True)
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if os.path.exists(target_dir) and os.path.isdir(target_dir):
        config_file = os.path.join(target_dir, 'config.json')
        if os.path.exists(config_file):
            print(f"âœ… æ¨¡å‹å·²å­˜åœ¨: {target_dir}")
            return target_dir
    
    print(f"ğŸ“¥ å¼€å§‹ä¸‹è½½åˆ°: {target_dir}")
    print("âš ï¸  SafeTensorsæ¨¡å‹ä½“ç§¯è¾ƒå¤§(10GB+)ï¼Œè¯·è€å¿ƒç­‰å¾…...")
    
    try:
        # è·å– HF Tokenï¼ˆç”¨äºå—é™æ¨¡å‹ï¼‰
        hf_token = os.getenv('HF_TOKEN', None)
        
        # ä¸‹è½½æ•´ä¸ªæ¨¡å‹ä»“åº“
        downloaded_path = snapshot_download(
            repo_id=model_id,
            local_dir=target_dir,
            token=hf_token,
            ignore_patterns=[
                "*.bin",  # å¿½ç•¥æ—§çš„PyTorchæ ¼å¼
                "*.msgpack",
                "*.h5",
                "*.ot",
                "*.onnx"
            ]
        )
        
        print(f"âœ… ä¸‹è½½å®Œæˆ: {downloaded_path}")
        return downloaded_path
        
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥: {e}")
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºè®¤è¯é—®é¢˜
        if "401" in str(e) or "403" in str(e) or "authentication" in str(e).lower():
            print("\nğŸ’¡ æç¤º: æ­¤æ¨¡å‹å¯èƒ½éœ€è¦è®¤è¯")
            print("è¯·è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡:")
            print("  1. è®¿é—® https://huggingface.co/settings/tokens")
            print("  2. åˆ›å»ºè®¿é—®ä»¤ç‰Œ")
            print("  3. æ·»åŠ åˆ° .env æ–‡ä»¶: HF_TOKEN=hf_your_token")
        
        raise



def check_model_exists(model_id: str, cache_dir: str) -> bool:
    """
    æ£€æŸ¥æ¨¡å‹æ˜¯å¦å·²ä¸‹è½½
    
    Args:
        model_id: æ¨¡å‹ID
        cache_dir: ç¼“å­˜ç›®å½•
        
    Returns:
        True if exists, False otherwise
    """
    model_path = os.path.join(cache_dir, model_id)
    return os.path.exists(model_path)
